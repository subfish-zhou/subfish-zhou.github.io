<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>GM on Subfish&#39;s Blog</title>
    <link>https://subfish-zhou.github.io/series/gm/</link>
    <description>Recent content in GM on Subfish&#39;s Blog</description>
    <image>
      <url>https://subfish-zhou.github.io/papermod-cover.png</url>
      <link>https://subfish-zhou.github.io/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 15 Apr 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://subfish-zhou.github.io/series/gm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>【Note】Diffusion</title>
      <link>https://subfish-zhou.github.io/posts/diffusion/diffusion/</link>
      <pubDate>Sat, 15 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://subfish-zhou.github.io/posts/diffusion/diffusion/</guid>
      <description>Diffusion 本文是Google发布的Understanding Diffusion Models: A Unified Perspective一文的笔记。
生成模型简介 给定我们关注的数据分布中的观测样本$x$，生成模型的目标是学习真实的数据分布$p(x)$。这样之后就可以从近似模型任意生成新的样本，以及使用近似模型来评估样本数据的概率。
目前有几个较重要的方向，下面简要概括：
生成对抗网络(GANs)模拟了一个复杂分布的抽样过程，它是通过对抗的方式学习的。 “基于似然的模型”，将高似然概率分配给观测样本。这包括自回归模型、正规流（normalizing flows）和变分自编码器(VAE)。 基于能量的模型，分布被学习为任意可变的能量函数，然后被归一化。 基于分数的模型，与能量模型很相关：它将基于能量的模型的分数作为神经网络来学习。 本文介绍扩散模型，它有基于似然和基于分数的解释。会非常数学，不过这些推导很细节，做好准备！
背景：ELBO，VAE和分层VAE 证据下界（Evidence Lower Bound） 对于很多模式中的数据，可以看作是由未知的隐变量$z$产生出的。把隐变量$z$和数据$x$建模成联合概率分布$p(x,z)$。基于似然的模型最大化所有观测$x$的可能性$p(x)$，可以用两种方法操纵联合分布排除隐变量$z$来恢复$p(x)$： $$p(x)=\int p(x,z)dz\tag{1}$$
$$p(x)=\frac{p(x,z)}{p(z|x)}\tag{2}$$
直接计算以及最大化似然函数$p(x)$是困难的，因为积分和条件概率的计算都比较难。我们用一个近似的变分分布$q_{\phi}(z|x)$来估计后验概率$p(z|x)$，然后去优化最大似然的对数（这里被称为证据Evidence）：
$$ \begin{aligned} \log p(\pmb{x}) &amp;amp;=\log p(\pmb{x}) \int q_{\phi}(\pmb{z} | \pmb{x}) d z \ &amp;amp;=\int q_{\pmb{\phi}}(\pmb{z} | \pmb{x})(\log p(\pmb{x})) d z \ &amp;amp;=\mathbb{E}{q{\phi}(\pmb{z} | \pmb{x})}[\log p(\pmb{x})] \ &amp;amp;=\mathbb{E}{q{\phi}(\pmb{z} | \pmb{x})}\left[\log \frac{p(\pmb{x}, \pmb{z})}{p(\pmb{z} | \pmb{x})}\right] \ &amp;amp;=\mathbb{E}{q{\phi}(\pmb{z} | \pmb{x})}\left[\log \frac{p(\pmb{x}, \pmb{z}) q_{\phi}(\pmb{z} | \pmb{x})}{p(\pmb{z} | \pmb{x}) q_{\phi}(\pmb{z} | \pmb{x})}\right] \ &amp;amp;=\mathbb{E}{q{\phi}(\pmb{z} | \pmb{x})}\left[\log \frac{p(\pmb{x}, \pmb{z})}{q_{\pmb{\phi}}(\pmb{z} | \pmb{x})}\right]+\mathbb{E}{q{\phi}(\pmb{z} | \pmb{x})}\left[\log \frac{q_{\phi}(\pmb{z} | \pmb{x})}{p(\pmb{z} | \pmb{x})}\right] \ &amp;amp;=\mathbb{E}{q{\phi}(\pmb{z} | \pmb{x})}\left[\log \frac{p(\pmb{x}, \pmb{z})}{q_{\pmb{\phi}}(\pmb{z} | \pmb{x})}\right]+D_{\mathrm{KL}}\left(q_{\phi}(\pmb{z} | \pmb{x}) | p(\pmb{z} | \pmb{x})\right) \ &amp;amp; \geq \mathbb{E}{q{\phi}(\pmb{z} | \pmb{x})}\left[\log \frac{p(\pmb{x}, \pmb{z})}{q_{\phi}(\pmb{z} | \pmb{x})}\right] \end{aligned} $$</description>
    </item>
    
  </channel>
</rss>
