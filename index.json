[{"content":"深入浅出扩散模型  本文是Google发布的Understanding Diffusion Models: A Unified Perspective一文的笔记。\n 生成模型简介 给定我们关注的数据分布中的观测样本$x$，生成模型的目标是学习真实的数据分布$p(x)$。这样之后就可以从近似模型任意生成新的样本，以及使用近似模型来评估样本数据的概率。\n目前有几个较重要的方向，下面简要概括：\n 生成对抗网络(GANs)模拟了一个复杂分布的抽样过程，它是通过对抗的方式学习的。 “基于似然的模型”，将高似然概率分配给观测样本。这包括自回归模型、正规流（normalizing flows）和变分自编码器(VAE)。 基于能量的模型，分布被学习为任意可变的能量函数，然后被归一化。 基于分数的模型，与能量模型很相关：它将基于能量的模型的分数作为神经网络来学习。  本文介绍扩散模型，它有基于似然和基于分数的解释。会非常数学，不过这些推导很细节，做好准备！\n背景：ELBO，VAE和分层VAE 证据下界（Evidence Lower Bound） 对于很多模式中的数据，可以看作是由未知的隐变量$z$产生出的。把隐变量$z$和数据$x$建模成联合概率分布$p(x,z)$。基于似然的模型最大化所有观测$x$的可能性$p(x)$，可以用两种方法操纵联合分布排除隐变量$z$来恢复$p(x)$： $$p(x)=\\int p(x,z)dz\\tag{1}$$\n$$p(x)=\\frac{p(x,z)}{p(z|x)}\\tag{2}$$\n直接计算以及最大化似然函数$p(x)$是困难的，因为积分和条件概率的计算都比较难。我们用一个近似的变分分布$q_{\\phi}(z|x)$来估计后验概率$p(z|x)$，然后去优化最大似然的对数（这里被称为证据Evidence）：\n$$ \\begin{aligned} \\log p(\\pmb{x}) \u0026amp;=\\log p(\\pmb{x}) \\int q_{\\phi}(\\pmb{z} | \\pmb{x}) d z \\ \u0026amp;=\\int q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})(\\log p(\\pmb{x})) d z \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}[\\log p(\\pmb{x})] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{p(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z}) q_{\\phi}(\\pmb{z} | \\pmb{x})}{p(\\pmb{z} | \\pmb{x}) q_{\\phi}(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})}\\right]+\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{q_{\\phi}(\\pmb{z} | \\pmb{x})}{p(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})}\\right]+D_{\\mathrm{KL}}\\left(q_{\\phi}(\\pmb{z} | \\pmb{x}) | p(\\pmb{z} | \\pmb{x})\\right) \\ \u0026amp; \\geq \\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\phi}(\\pmb{z} | \\pmb{x})}\\right] \\end{aligned} $$\n因为$q_{\\phi}(z|x)$和$p(z|x)$的DL散度$D_{\\mathrm{KL}}\\left(q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x}) | p(\\pmb{z} | \\pmb{x})\\right) \\geq 0$，最后一行得到的期望被称为证据下界（ELBO），透过不等式，最大化ELBO就等价于最大化似然和最小化DL散度（注意，ELBO是关于$\\phi$的函数）。\n如何实现ELBO的最大化，亦即如何实现一个好的$q_{\\phi}(z|x)$，将我们引到了VAE之中。\n变分自编码器（Variational AutoEncoder） 自编码器是一个学习输入数据$x$，获得中间表示$z$，然后再拿来预测$x$自己的模型。$q_{\\phi}(z|x)$就是编码器，$p_{\\theta}(x|z)$就是解码器。说它是变分的，是因为$q_{\\phi}(z|x)$从一族由$\\phi$参数化的隐后验分布中优化而来。\n","permalink":"https://subfish-zhou.github.io/posts/diffusion/diffusion/","summary":"我希望这是全网最好的扩散模型理论入门教程，代码实践见下期","title":"【笔记】深入扩散模型教程"},{"content":"一般视觉语言模型  本文译自Lilian Weng的文章Generalized Visual Language Models，在此感谢她的辛勤创作，献上花花🌸。由于翻译量较大，机翻了部分段落；还有一些术语不知道怎么翻译或者翻译出来怪怪的，都附上英文原文。译者能力有限，恳请读者海涵。\n 处理图像生成文本，例如看图说话和视觉QA，已经研究了好多年。传统上，这种系统依赖于对象检测网络作为视觉编码器来捕获视觉特征，然后通过文本解码器产生文本。本文总结了现今的不少论文，专注于解决视觉语言任务的一种方法，即扩展预训练的一般语言模型，使其能够处理视觉信号。\n我将这种视觉语言模型(VLMs)大致分为四类:\n 将图像转换为可与文本token嵌入联合训练的嵌入特征。 学习好的图像嵌入，可以作为固定的、预训练的语言模型的前缀。 使用特别设计的交叉注意机制将视觉信息融合到语言模型的各个层中。 结合视觉和语言模型，无需任何训练。  图像和文本联合训练 将视觉信息融合到语言模型中的一种直接方法是将图像视为普通的文本token，并在文本和图像的联合表示序列上训练模型。准确地说，图像被分成许多小块(patch)，每个小块被视为输入序列中的一个token。\nVisualBERT(Li et al. 2019)将文本输入和图像区域都输入到BERT中，这样它就能够通过自注意力发现图像和文本之间的内部对齐。\n  图1：VisualBERT是结合文本和图像嵌入进行训练的。图源：[(Li et al. 2019)](https://arxiv.org/abs/1908.03557)\n  类似于BERT中的文本嵌入，VisualBERT中每个视觉嵌入也是由三种类型的嵌入加和而成的，分别是tokenized特征$f_o$，识别嵌入(segment embedding) $f_s$和位置嵌入$f_p$。具体地：\n $f_o$是觉特征向量，由CNN在一个边界区域上计算得来； $f_s$是识别嵌入，用于识别嵌入来自视觉还是文本； $f_p$是位置嵌入，用于给边界区域排顺序。  该模型在MS COCO图像标题数据集上训练，文本和图像都作为输入来预测文本描述(caption)，使用两个基于视觉的语言模型目标：\n 含图像的掩码语言模型MLM。该模型需要预测文本tokens中的掩码，而图像嵌入始终保持不掩码。 句子-图像预测。提供一张图片和两个描述句子，其中一个句子是图片相关的，另一个句子有50%概率相关50%概率随机。模型被要求区分这两种情况。  根据消融实验，最重要的配置是将视觉信息早早融合到transformer层，并在COCO caption数据集上预训练模型。从一个预训练好的BERT初始化以及采用句子-图像预测训练目标，带来的影响相对较小。\n  图2：VisualBERT在NLVR上的消融实验结果。图源：[(Li et al. 2019)](https://arxiv.org/abs/1908.03557)\n  VisualBERT在NLVR和Flickr30K上的性能优于SoTA，但在VQA上仍与SoTA存在一定的性能差距。\nSimVLM(Simple Visual Language Model；Wang et al. 2022)是一个简单的前缀语言模型，其中前缀序列像BERT一样用双向注意力处理，但主输入序列像GPT一样只有因果注意力。图像被编码为前缀token，这样模型就可以完全采纳视觉信息，然后以自回归的方式生成相关的文本。\n受ViT和CoAtNet的启发，SimVLM将图像分割成更小的块，并展成一维小块序列。他们使用由ResNet的前3个块组成的卷积步骤来提取包含上下文信息的小块，这种设置被发现比平凡的线性投影更好。\n  图3：SimVLM的训练架构，其中图像小块由交叉注意力编码器处理，文本解码器具有因果注意力。图源：[Wang et al. 2022](https://arxiv.org/abs/2108.10904)\n  SimVLM的训练数据包括来自ALIGN(Jia et al. 2021)的大量图像-文本对和来自C4数据集(Raffel et al. 2019)的纯文本数据。他们在每个小批量(batch)中混合了两个预训练数据集，包含4096个图像-文本对(ALIGN)和512个纯文本文档(C4)。\n根据消融实验，同时拥有图像-文本和纯文本数据进行训练是很重要的。前缀语言模型目标优于span corruption和普通语言模型。\n  图4：SimVLM在VQA上的消融实验结果。图源:[(Wang et al. 2022)](https://arxiv.org/abs/2108.10904)\n  ","permalink":"https://subfish-zhou.github.io/posts/generalized_visual_lm/","summary":"综述","title":"【翻译】一般视觉语言模型"},{"content":"自动形式化的问题背景  Therefore it is natural to ask: Will we ever arrive at the point where an AI agent can learn to do reasoning as well as the best humans in the world in most established domains of mathematics. \u0026mdash;- Christian Szegedy\n 本文参考了A Promising Path Towards Autoformalization and General Artificial Intelligence\n总所周知，深度学习的推理能力不太行，我们很自然地想到可以在数学上测试推理AI。但在进入问题之前还存在两个明显的困难：（1）人类用自然语言表达数学知识，自然语言的模糊性让计算机很难用机械化的方法理解和验证；（2）人类的数学知识分散在浩如烟海的论文中，难以生成有效的数据集供系统训练使用。\n与此同时，人类的基础数学发展也遇到了明显的瓶颈，人类的数学论文复杂程度越来越高导致检验其正确性的难度越来越高，常常出现一篇论文审稿时间极长甚至无法审稿的局面。\n应对这些问题，一些数学家开发了形式化数学语言来表征数学知识，这些形式化语言表征的命题和证明过程可以通过固定的程序来验证。这些数学家希望通过计算机来验证人类数学知识的正确性，促进人类数学的发展，但是将人类数学翻译成形式化语言的工作量十分巨大，目前主要的形式化语言的数学定理库的内容十分有限，甚至不能完整包含大学本科数学的内容。\n因此，开发一种数学自动形式化算法的需求显得尤为迫切，一方面可以为数学推理人工智能提供大量表述清晰的数学知识库，另一方面可以大规模验证人类数学知识的正确性。\n简史 自动形式化的想法是由John McCarthy在1961年首次提出的。另一个早期尝试是1990年Donald Simons的博士论文。2004年Claus Zinn的博士论文中进行了第一次彻底的研究。这些工作并没有产生哪怕是部分实用的解决方案。\nJosef Urban在21世纪初开始研究这个课题。他设计了第一个大规模的大型理论推理基准，其动机是认为在大型数学事实知识库中进行推理是任何自动形式化系统的关键组成部分。2007年，他发表了用于大型理论推理的开创性的MaLARea系统。从那时起，他和Cezary Kaliszyk一直在带头研究大型理论的推理和自动格式化。\n 参考 [1]First experiments with neural translation of informal to formal mathematics [2]Exploration of neural machine translation in autoformalization of mathematics in Mizar\n AI技术在数学上的部分应用   GPT-f系列模型在自动推理上的进展：可以做一部分IMO难度的题目了。\n 参考 [1]Generative Language Modeling for Automated Theorem Proving [2]Mathematical Reasoning via Self-supervised Skip-tree Training [3]Proof Artifact Co-training for Theorem Proving with Language Models [4]Formal Mathematics Statement Curriculum Learning\n   机器学习引导人类直觉，使用聚类算法捕捉纽结理论和对称群论的例子中的模式，由此发现了两个新的定理。\n 参考 [1]Advancing mathematics by guiding human intuition with AI\n   一种神经网络求解器，可以利用Python的符号计算包sumpy求解大学本科的数学题目。\n 参考 [1]A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More\n   可能的技术路径 我目前在做一个自然语言对应到Lean的数据集，当获得数据集之后，分别设计自动翻译定理和证明的方法。定理的翻译大体上可以使用transformer类模型；证明的翻译应该是一个强化学习任务，暂时不是很明确。\n或者可以尝试无监督方法。参考Unsupervised Translation of Programming Languages\n","permalink":"https://subfish-zhou.github.io/posts/autoformalization/","summary":"一个新课题","title":"数学自动形式化：通向自动推理之路"},{"content":"深度学习时代的自动定理证明简介 基础数学终于等来了可以由计算机辅助来大量提高生产力的一天。在最近的几年里，自动定理证明（automatic theorem proving，ATP）领域通过引入深度学习和强化学习，尤其是凭借大语言模型（Large language model, LLM）的飞速进展，产生了大量令人惊叹的工作。尽管离让计算机自己证出黎曼猜想这样的大命题还稍显遥远，但我们已经可以可预见地谈论自动定理证明将如何帮助人类数学家完成工作了。本文致敬（蹭热度x）颇有影响力的 @彭柯尧 《计算机辅助证明简介》一文，该文所介绍的形式化数学语言也作为本文的理论背景，现代ATP系统基本都是在formal language的基础上开展的，并表现为Lean中的tactic或Isabelle中的hammer等，因此建议没有阅读过该文的本文读者先行参考。\n本文主要介绍当下融入深度学习，强化学习的自动定理证明问题进展（下文简写为DL+ATP，但要注意RL也是其中关键），DL+ATP发展迅猛，但我不希望本文太早过时，因此不会介绍太多的技术性内容，我将会用另一篇持续更新的综述来跟进领域进展。本文主要介绍想法、意义，澄清误解和解答疑惑，目的是给数学、计算机以及更多领域的爱好者和研究者展示这个新兴且蓬勃又意义深远的领域。\n本文会从以下几个问题展开（但答案是相互交织的，因此目录并不按此组织）：\n 为什么定理可以被自动证明； DL+ATP是怎样工作的； 当前的自动证明领域取得了哪些成果； 数学家希望从自动证明中得到什么； 自动定理证明会带来哪些影响； 初学者如何入门DL+ATP。  先展示一些目前的成果：截至本文写作时间2023.7.1，最有代表性和影响力的DL+ATP工作HyperTree Proof Search for Neural Theorem Proving（2022.5.23，介绍见Meta Blog，论文，下文简写为HTPS），自动解决了10道国际数学奥赛（IMO）的题目，例如其中一道题是这样：\n （IMO1964p1_2）7不能整除$2^n+1$. 证明：首先观察到$2^3 \\equiv 1(\\text{mod} 7)$, 所以对于$k=1,2,3,\u0026hellip;, 2^{3k}\\equiv 1 (\\text{mod} 7), 2^{3k}+1\\equiv 2( \\text{mod } 7), 2^{3k+1}+1\\equiv 3( \\text{mod } 7), 2^{3k+2}+1\\equiv 5( \\text{mod } 7)$, 所以对于所有自然数n，7不能整除$2^n+1$.\n 这道题在Lean中的形式化表达，以及AI给出的自动证明过程如下：\nimport data.nat.prime\rtheorem imo_1964_p1_2 (n : ℕ) :\r¬ 7 ∣ (2 ^ n + 1) :=\rbegin\rrw nat.dvd_iff_mod_eq_zero,\rrewrite [nat.add_mod, nat.mod_eq_of_lt],\robviously,\rapply nat.strong_induction_on n,\rinduction n,\r{\rintros n IH,\rcases n,\rnorm_num,\rcases n,\rnorm_num,\rrw [nat.succ_eq_add_one, pow_succ],\rrw [nat.succ_eq_add_one, pow_succ],\rinduction n,\rnorm_num,\rrw [nat.succ_eq_add_one, pow_succ],\rnorm_num [nat.mul_mod, ←mul_assoc],\rcontrapose! IH,\rrefine ⟨n_n, nat.lt_succ_iff.mpr _, IH⟩,\rexact nat.le_succ_of_le (nat.le_succ _),\r},\rexact n_ih,\rend 这个自动生成的证明(经大量的简化后)翻译成自然语言大概是：\n 原问题等价于$2^n+1$除以7的余数不等于0, 从而要么$2^n$除以7的余数加上1除以7的余数不等于0, 而这是显然的, 要么$2^n$除以7的余数加上1除以7的余数小于7. 对此命题使用强归纳法, 即证明 ∀(n :ℕ),(∀(m :ℕ),m \u0026lt; n → 2 ^ m % 7 + 1 \u0026lt; 7) → 2 ^ n % 7 + 1 \u0026lt; 7 再进一步对n归纳, 1)已知∀ (m : ℕ), m \u0026lt; 0 → 2 ^ m % 7 + 1 \u0026lt; 7 证明 2 ^ 0 % 7 + 1 \u0026lt; 7 : 这是显然的 2)已知∀ (m : ℕ), m \u0026lt; n + 1 → 2 ^ m % 7 + 1 \u0026lt; 7 证明 2 ^ (n + 1) % 7 + 1 \u0026lt; 7 再对1)进一步对n归纳 1.0) 已知∀ (m : ℕ), m \u0026lt; 1 → 2 ^ m % 7 + 1 \u0026lt; 7 证明 2 ^ 1 % 7 + 1 \u0026lt; 7 : 这是显然的 1.1) 已知∀ (m : ℕ), m \u0026lt; n + 1 + 1 → 2 ^ m % 7 + 1 \u0026lt; 7 证明 2 ^ (n + 1 + 1) % 7 + 1 \u0026lt; 7 再对1.1)进一步对n归纳 1.1.0) 已知∀ (m : ℕ), m \u0026lt; 1 + 1 → 2 ^ m % 7 + 1 \u0026lt; 7 证明 2 ^ (1 + 1)% 7 + 1 \u0026lt; 7 : 这是显然的 1.1.1) 已知∀ (m : ℕ), m \u0026lt; n + 1 + 1 + 1→ 2 ^ m % 7 + 1 \u0026lt; 7 证明 2 ^ (n + 1 + 1 + 1) % 7 + 1 \u0026lt; 7 至此, 2 ^ (n + 1 + 1 + 1) % 7 + 1 = 8 * 2 ^ n % 7 + 1 = (7 + 1) * 2 ^ n % 7 + 1 = 2 ^ n % 7 + 1, 而这正好是最原始的归纳假设, 由此就得到了证明.\n 如果我们仔细观察会发现，这个证明经过极度暴力的，具有浓浓机械风的手段，实际上也变相实现了人类证明中“首先观察到”的这一“奇思妙想”（TL;DR,主要体现在最后一步）。要指出的是，这个证明并不是通过符号主义的固定算法，例如SAT或者吴方法等算法生成出来的，而的确是通过类似人类数学家的思考方式，在证明树上主动搜索出来的（后文会简单介绍算法思想）。不过虽说这个算法的原理是接近人类思考的，但这个证明结果确实不像人类写出的，冗长繁琐不雅观，数学家看后大概会觉得饭碗尚牢。但这个结果也不由得让我们反思，我们可能确实已经来到了可以讨论AI自动证明定理的时刻了，虽然此题的“观察”比较简单，略经训练的学生便能掌握，但被认为更深刻的定理证明中的“艺术”真的就是某种高不可攀的人类智慧之光，不容AI染址的圣地吗？ 注：你可能会听说过LeanDojo（被新智元诈骗号吹上天的工作），论文中未见它的性能较Hypertree更强。下文会讲到。\n自动证明是如何可能的 自动定理证明是最早和最传统的人工智能问题之一，早在1954年，Martin Davis就开发了第一个定理证明程序，证明了“两个偶数的和是偶数”；而Allen Newell和Herbert Simon在1956年的不朽的达特茅斯会议上展示的Logic theorist程序证明了罗素与怀特海的名著Principia Mathematica中的一大半定理，甚至有些比作者的证明更简洁，由此开创了符号学派。这些早期工作给计算机科学带来的影响不胜枚举，例如形式验证、专家系统和知识图谱都是它的后裔，甚至对于SAT的研究开启了计算复杂性这一学科。符号主义路线的顶峰是Vampire程序，至今仍在更新，并在形式验证等领域继续发挥作用。但是对于原始的自动证明数学定理的目标，符号主义路线不可避免地受限于组合爆炸，因而长期停滞，无法运用到数学家的实践中。在统计学习时代，有将支持向量机等模型用于构造证明搜索启发式的研究，但收效不佳。在最近几年，当深度学习等工具加入到自动定理证明后，我们终于有希望通过深度学习开发出足够优秀的启发式来克服组合爆炸，甚至实现近乎人类思考方式的定理证明算法了。\n在介绍DL+ATP是如何可能的之前，先介绍一下“证明”作为一种计算任务具有怎样的结构。这里并不需要涉及证明论中诸如相继式演算的写法，也可以无视类似归结原则等演算的具体规则，单就从最抽象的任务结构上来看的话，我们可以把证明看做一棵搜索树。类比于在围棋游戏中，每一步都有若干选点可下，而最终会有几条道路通向胜利，定理证明也可以看做在每一个命题状态（在证明论以及ITP系统中，状态就是context和goal）下都可以把若干定理或者证明技巧应用到当前状态上，使它变成下一个状态，并最终会有几条道路通向证明完成。我们用一个例子来展示搜索树和最简单的启发式是如何工作的：证明“sqrt(2)是无理数”。如果读者熟悉Lean等ITP系统是如何工作的，这个例子可能会更好理解。\n拿到这个命题，我们有几种选择，例如我们可以尝试归纳法，然后发现命题中并不含有能归纳的东西，因此没有办法执行归纳法。那么也许可以试试反证法？随后我们发现反证法的确改变了问题的状态，再把定义还原回去后（Lean中就是rewrite了一下定义）我们就获得了一条新命题“存在m,n，m,n互素且sqrt(2)=m/n”并且目标变成了推出矛盾。接下来又会有很多选择，比如把“等式两边可以同时+1”应用上去，或者把“等式两边可以同时×2”应用上去，也可以把“等式两边可以同时平方”应用上去。如果我们不预先知道答案的话，这里其实是有无数种选择的，也就是这棵搜索树无限宽。（在Lean中表现为，我们有一大堆备选tactic和可以rw的定理）此时已经初见“组合爆炸”的端倪了，自动证明算法不可能遍历这样多的定理，那么就需要一些启发式帮助挑选接下来的搜索分支。比如可以用命题里包含多少符号来定义一个“命题复杂程度”，搜索时尽量防止命题复杂程度上升。这样一来算法可能会放弃尝试“两边+1”，因为让命题变得更复杂了。一个没那么朴素，在此刻更有针对性的启发式可能是“对符号的熟悉程度”，例如算法发现自己对根号sqrt知之甚少，只有很少的定理可以处理它，而更多定理可以处理整数和有理数，那么它会倾向于找一个办法消除根号，在此刻就是使用“两边平方”。这样一来我们又（跳了一步）把原命题变成“存在m,n，m,n互素且2=m²/n²”。随后算法又觉得自己可能对分数也没那么熟悉，因此又把n²乘了过去，于是这个启发式的作用可能到此为止，算法再去考虑别的搜索技巧。\n由于Gödel不完备性定理，Tarski的一阶实数可判定但超指数算法，以及SAT的NPC性质，可以认为不存在可以指导一切证明的“终极启发式”，能完全解决初等几何问题的吴方法只是极少数的特例，在一般的问题中一个启发式只能应用于一小块很狭窄的问题形态。符号主义时代的定理证明器就是启发式的大比拼，谁的启发式设计更精巧，谁能更好协调多个启发式，谁就能取得更好的性能。但数学问题花样百出，即使经历了几十年的辛勤工作，这些证明器在高中数学面前都望而却步，更不用说跟进现代数学了。\n实际上，AI的其他领域也遇到了类似的情境，例如图像识别领域也在依赖人类手工设计特征，但在Imagenet数据集上的性能一直在50%附近浮动。2014年深度学习横空出世，瞬间改变了图像识别领域的面貌，而如今深度学习也开始在定理证明领域崭露头角。\n为什么这一切可能发生？你可能会感觉到，符号启发式的设计或许符合人类在一部分情形下的一些想法，但并没有捕捉到人类的数学实践中最关键的一环：人类经历了相当大量的数学训练，积累了大量的经验，甚至有些时候这些经验难以言喻，“我不知道我是怎么想出来的，但是我就是想到了”。深度学习恰恰能够让算法自己积累经验，当它做过好多好多题，它在遇到一个新题目的时候，就会回想自己是否见过类似的“题型”，并找到与当时训练时的类似做法。\n我们可以用蒙特卡洛搜索树来描述被统计学增强过的定理证明模型。蒙特卡洛搜索树在每个分支上会给出一个概率，例如选择“反证法”的概率是0.9，选择“归纳法”的概率是0.09，另外一些分支占据了0.01。搜索时依照概率大小优先搜索，这样就避免了遍历所有分支。在符号主义时代实际上多个启发式的联合作用也会使用这个模型，但在固定的符号启发式组合下这个概率一般是固定的，但现在我们可以通过训练调整这些概率（这其实是强化学习的技术），例如算法在搜索一个0.9概率的分支后发现这个分支的后代并不能成功证明定理，说明算法对这个方法的成功信心估计有误，后面就会适当减小这个概率避免重蹈覆辙（只是减小而不是彻底杀死它的原因是，这个分支的后代也许并没有被完全遍历因此并不能完全确定它就是不行的）。本文开头介绍的HTPS算法在此思路上更进一步，它发现人遇到一些题型时会有相对固定的解题套路，也就是一连串的定理应用步骤可能会反复出现，中间可能只有少数参数是根据具体情况变化。于是它将这些套路打包成树，这些树成为了每一个搜索步处的备选项，从而不必逐个搜索下一步需要应用的定理。这个“树上加树”的想法即为“Hypertree”的由来。\n仅有蒙特卡洛搜索树的概率反馈机制是不够的，因为树过于庞大（无限大），多半是找不到解的，因此通过深度神经网络来从训练中积累经验也是必须的，在蒙特卡洛树模型中表现为给定一个足够好的初始概率。深度学习最擅长的事就是捕捉特征，例如图像中猫和狗的特征，或者英语和汉语中共通的特征。定理中也包含一些特征，细节的例子过于琐碎，从最抽象的意义上讲，分析学的定理更可能用分析的方式证明，代数学的定理更可能用代数的工具解决，即使是真的用分析方式证明的代数学定理，我们也能感受到某些“分析学的味道”在其中，这些其实也是一种对搜索方向的限制。深度神经网络通过在数据中自行发现命题或证明过程中的特征，并在遇到具有类似特征的命题时采用对应的证明过程，就实现了在做题中“积累经验”的效果。\n不同任务数据中的特征往往有各自的独特性，而我们对应地设计不同结构的网络能更好地提取到它们，例如我们用卷积神经网络来捕捉图像中的特征。那么我们应当设计怎样的深度神经网络来提取证明中的特征呢？一些较早期的工作曾经将证明看作序列或者图，相应地使用了RNN或GNN来处理它，但收效甚微。近期大语言模型获得的成功促使我们把证明看作是一种语言，进而可以用语言模型来捕捉特征。以HTPS为代表的GPT-f系列工作使用的是近似GPT2的语言模型，而最近的LeanDojo使用的是T5模型。这些语言模型的工作细节以及不同的语言模型在定理证明上的区别并不在本文主题之内，读者可以自行查询资料，本文随后的技术综述会介绍这些内容。\n但是我们也看到在LLM上训练定理证明的效果并不足够好。LeanDojo的部分工作表明，即使在充分的prompt之下GPT4也无法证明像Stirling公式等比较复杂的数学定理。原因可能是定理证明数据太少了，也没有找到最合适的调教LLM的方法，导致没有充分发挥LLM的能力；也有可能是定理证明任务本身具有比语言更丰富复杂精细的结构，LLM的暴力美学并不足以征服定理证明的艰巨任务。但任务虽然艰巨却并不像几十年前那样神秘，人类对证明甚至推理的机制已经积累了丰富的认识，只是尚需投入海量的工作。\n本节最后回应一个常见的质疑观点：\n 因为哥德尔不完备性定理（或者别的什么理由），机器永远无法证明数学定理，因为人是不受哥德尔定理束缚的，所以定理证明只能由人来完成。\n 回应：哥德尔自己其实也有这种观点。但是这种观点意味着人是一个谕示机（oracle Turing machine），即一个可以运行无限时间的图灵机，并可以穿梭时间告知当下时刻的图灵机无限时间后的运行结果。根据丘奇-图灵论题，谕示机并不是我们这个宇宙中能够实现的计算模型。对应于定理证明任务，这意味着人可以不耗时间地遍历所有的可能性并找到正确的定理应用之，看上去人类的“灵感”具备类似的性质，但这实际上意味着能找到一个人从未经过数学训练，仅凭神仙托梦便可发现证明。我还是更愿意相信灵感源于某种具体的计算机制，尽管目前它只被深度学习和强化学习等理论不完整地描述。\n","permalink":"https://subfish-zhou.github.io/posts/dlatp/","summary":"科普","title":"深度学习时代的自动定理证明简介"}]