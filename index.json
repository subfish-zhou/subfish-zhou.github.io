[{"content":"My Profile Hi I\u0026rsquo;m Subfish Zhou! Studying AI4Math. My interest in AI4MATH began during my sophomore year, when realized the potential and profound impact of using AI to solve mathematical problems. The AI4MATH technology contributes to our understanding of intelligence, and the technology can directly increase human knowledge of mathematics, physics, and other natural sciences. Imagine how the world would change if AI could help us solve Riemann Hypothesis or create the controlled fusion technology!\nMy work Article:\nIntroduction of Deep Learning for Automatic Theorem Proving\nTheorem proving in Lean4 (translation)\nMathematics in Lean4 (translation, in progress)\nReport:\nReview of DL4ATP (in progress). In Swarma Club.\nProgress in automatic theorem proving of LLM . In The First Type Theory Summer School.\nHow to think about a thinking machine. In Geek College.\nConsciousness, knowledge, ways of thinking, and general artificial intelligence . In Geek College.\n","permalink":"https://subfish-zhou.github.io/posts/myprofile/","summary":"Who is Subfish","title":"My Profile"},{"content":"Diffusion 本文是Google发布的Understanding Diffusion Models: A Unified Perspective一文的笔记。\n生成模型简介 给定我们关注的数据分布中的观测样本$x$，生成模型的目标是学习真实的数据分布$p(x)$。这样之后就可以从近似模型任意生成新的样本，以及使用近似模型来评估样本数据的概率。\n目前有几个较重要的方向，下面简要概括：\n生成对抗网络(GANs)模拟了一个复杂分布的抽样过程，它是通过对抗的方式学习的。 “基于似然的模型”，将高似然概率分配给观测样本。这包括自回归模型、正规流（normalizing flows）和变分自编码器(VAE)。 基于能量的模型，分布被学习为任意可变的能量函数，然后被归一化。 基于分数的模型，与能量模型很相关：它将基于能量的模型的分数作为神经网络来学习。 本文介绍扩散模型，它有基于似然和基于分数的解释。会非常数学，不过这些推导很细节，做好准备！\n背景：ELBO，VAE和分层VAE 证据下界（Evidence Lower Bound） 对于很多模式中的数据，可以看作是由未知的隐变量$z$产生出的。把隐变量$z$和数据$x$建模成联合概率分布$p(x,z)$。基于似然的模型最大化所有观测$x$的可能性$p(x)$，可以用两种方法操纵联合分布排除隐变量$z$来恢复$p(x)$： $$p(x)=\\int p(x,z)dz\\tag{1}$$\n$$p(x)=\\frac{p(x,z)}{p(z|x)}\\tag{2}$$\n直接计算以及最大化似然函数$p(x)$是困难的，因为积分和条件概率的计算都比较难。我们用一个近似的变分分布$q_{\\phi}(z|x)$来估计后验概率$p(z|x)$，然后去优化最大似然的对数（这里被称为证据Evidence）：\n$$ \\begin{aligned} \\log p(\\pmb{x}) \u0026amp;=\\log p(\\pmb{x}) \\int q_{\\phi}(\\pmb{z} | \\pmb{x}) d z \\ \u0026amp;=\\int q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})(\\log p(\\pmb{x})) d z \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}[\\log p(\\pmb{x})] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{p(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z}) q_{\\phi}(\\pmb{z} | \\pmb{x})}{p(\\pmb{z} | \\pmb{x}) q_{\\phi}(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})}\\right]+\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{q_{\\phi}(\\pmb{z} | \\pmb{x})}{p(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})}\\right]+D_{\\mathrm{KL}}\\left(q_{\\phi}(\\pmb{z} | \\pmb{x}) | p(\\pmb{z} | \\pmb{x})\\right) \\ \u0026amp; \\geq \\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\phi}(\\pmb{z} | \\pmb{x})}\\right] \\end{aligned} $$\n因为$q_{\\phi}(z|x)$和$p(z|x)$的DL散度$D_{\\mathrm{KL}}\\left(q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x}) | p(\\pmb{z} | \\pmb{x})\\right) \\geq 0$，最后一行得到的期望被称为证据下界（ELBO），透过不等式，最大化ELBO就等价于最大化似然和最小化DL散度（注意，ELBO是关于$\\phi$的函数）。\n如何实现ELBO的最大化，亦即如何实现一个好的$q_{\\phi}(z|x)$，将我们引到了VAE之中。\n变分自编码器（Variational AutoEncoder） 自编码器是一个学习输入数据$x$，获得中间表示$z$，然后再拿来预测$x$自己的模型。$q_{\\phi}(z|x)$就是编码器，$p_{\\theta}(x|z)$就是解码器。说它是变分的，是因为$q_{\\phi}(z|x)$从一族由$\\phi$参数化的隐后验分布中优化而来。\n","permalink":"https://subfish-zhou.github.io/posts/diffusion/diffusion/","summary":"summary","title":"【Note】Diffusion"},{"content":"General Visual Language Model 本文译自Lilian Weng的文章Generalized Visual Language Models，在此感谢她的辛勤创作，献上花花🌸。由于翻译量较大，机翻了部分段落；还有一些术语不知道怎么翻译或者翻译出来怪怪的，都附上英文原文。译者能力有限，恳请读者海涵。\n处理图像生成文本，例如看图说话和视觉QA，已经研究了好多年。传统上，这种系统依赖于对象检测网络作为视觉编码器来捕获视觉特征，然后通过文本解码器产生文本。本文总结了现今的不少论文，专注于解决视觉语言任务的一种方法，即扩展预训练的一般语言模型，使其能够处理视觉信号。\n我将这种视觉语言模型(VLMs)大致分为四类:\n将图像转换为可与文本token嵌入联合训练的嵌入特征。 学习好的图像嵌入，可以作为固定的、预训练的语言模型的前缀。 使用特别设计的交叉注意机制将视觉信息融合到语言模型的各个层中。 结合视觉和语言模型，无需任何训练。 图像和文本联合训练 将视觉信息融合到语言模型中的一种直接方法是将图像视为普通的文本token，并在文本和图像的联合表示序列上训练模型。准确地说，图像被分成许多小块(patch)，每个小块被视为输入序列中的一个token。\nVisualBERT(Li et al. 2019)将文本输入和图像区域都输入到BERT中，这样它就能够通过自注意力发现图像和文本之间的内部对齐。\n图1：VisualBERT是结合文本和图像嵌入进行训练的。图源：[(Li et al. 2019)](https://arxiv.org/abs/1908.03557)\n类似于BERT中的文本嵌入，VisualBERT中每个视觉嵌入也是由三种类型的嵌入加和而成的，分别是tokenized特征$f_o$，识别嵌入(segment embedding) $f_s$和位置嵌入$f_p$。具体地：\n$f_o$是觉特征向量，由CNN在一个边界区域上计算得来； $f_s$是识别嵌入，用于识别嵌入来自视觉还是文本； $f_p$是位置嵌入，用于给边界区域排顺序。 该模型在MS COCO图像标题数据集上训练，文本和图像都作为输入来预测文本描述(caption)，使用两个基于视觉的语言模型目标：\n含图像的掩码语言模型MLM。该模型需要预测文本tokens中的掩码，而图像嵌入始终保持不掩码。 句子-图像预测。提供一张图片和两个描述句子，其中一个句子是图片相关的，另一个句子有50%概率相关50%概率随机。模型被要求区分这两种情况。 根据消融实验，最重要的配置是将视觉信息早早融合到transformer层，并在COCO caption数据集上预训练模型。从一个预训练好的BERT初始化以及采用句子-图像预测训练目标，带来的影响相对较小。\n图2：VisualBERT在NLVR上的消融实验结果。图源：[(Li et al. 2019)](https://arxiv.org/abs/1908.03557)\nVisualBERT在NLVR和Flickr30K上的性能优于SoTA，但在VQA上仍与SoTA存在一定的性能差距。\nSimVLM(Simple Visual Language Model；Wang et al. 2022)是一个简单的前缀语言模型，其中前缀序列像BERT一样用双向注意力处理，但主输入序列像GPT一样只有因果注意力。图像被编码为前缀token，这样模型就可以完全采纳视觉信息，然后以自回归的方式生成相关的文本。\n受ViT和CoAtNet的启发，SimVLM将图像分割成更小的块，并展成一维小块序列。他们使用由ResNet的前3个块组成的卷积步骤来提取包含上下文信息的小块，这种设置被发现比平凡的线性投影更好。\n图3：SimVLM的训练架构，其中图像小块由交叉注意力编码器处理，文本解码器具有因果注意力。图源：[Wang et al. 2022](https://arxiv.org/abs/2108.10904)\nSimVLM的训练数据包括来自ALIGN(Jia et al. 2021)的大量图像-文本对和来自C4数据集(Raffel et al. 2019)的纯文本数据。他们在每个小批量(batch)中混合了两个预训练数据集，包含4096个图像-文本对(ALIGN)和512个纯文本文档(C4)。\n根据消融实验，同时拥有图像-文本和纯文本数据进行训练是很重要的。前缀语言模型目标优于span corruption和普通语言模型。\n图4：SimVLM在VQA上的消融实验结果。图源:[(Wang et al. 2022)](https://arxiv.org/abs/2108.10904)\n","permalink":"https://subfish-zhou.github.io/posts/generalized_visual_lm/","summary":"summary","title":"【Translate】General Visual Language Model"},{"content":"Automatic formalization of mathematics: a path to automatic reasoning Therefore it is natural to ask: Will we ever arrive at the point where an AI agent can learn to do reasoning as well as the best humans in the world in most established domains of mathematics. \u0026mdash;- Christian Szegedy\nThis paper refers to A Promising Path Towards Autoformalization and General Artificial Intelligence\nIt is well known that deep learning is not very good at reasoning, and it is natural for us to think that we can test reasoning AI mathematically. But before entering into the problem, there are two obvious difficulties: (1) humans express mathematical knowledge in natural language, and the fuzziness of natural language makes it difficult for computers to understand and verify it in a mechanized way; (2) Human mathematical knowledge is scattered in a vast number of papers, and it is difficult to generate effective data sets for system training.\nAt the same time, the development of human basic mathematics has also encountered an obvious bottleneck, the complexity of human mathematical papers is getting higher and higher, resulting in the difficulty of verifying their correctness, often a paper review time is extremely long or even unable to review the situation.\nIn response to these problems, some mathematicians have developed formal mathematical languages to represent mathematical knowledge, and these formal languages represent propositions and proof processes that can be verified by fixed procedures. These mathematicians hope to verify the correctness of human mathematical knowledge and promote the development of human mathematics through computers, but the workload of translating human mathematics into formal language is very huge, and the content of the mathematical theorem library of the main formal language is very limited, and even cannot completely include the content of undergraduate mathematics.\nTherefore, the need to develop a mathematical automatic formalization algorithm is particularly urgent, on the one hand, it can provide a large number of clear mathematical knowledge base for mathematical reasoning artificial intelligence, on the other hand, it can verify the correctness of human mathematical knowledge on a large scale.\n简史 The idea of automatic formalization was first proposed by John McCarthy in 1961. Another early attempt was Donald Simons\u0026rsquo; doctoral thesis in 1990. The first thorough study was carried out in Claus Zinn\u0026rsquo;s PhD thesis in 2004. None of this work has produced even a partially practical solution.\nJosef Urban began working on the subject in the early 2000s. He designed the first large-scale benchmark for large-scale theoretical reasoning, motivated by the belief that reasoning in a large knowledge base of mathematical facts is a key component of any automated formalization system. In 2007, he published the groundbreaking MaLARea system for large-scale theoretical reasoning. Since then, he and Cezary Kaliszyk have been spearheading research into inference and automatic formatting of large theories.\nRef. [1]First experiments with neural translation of informal to formal mathematics [2]Exploration of neural machine translation in autoformalization of mathematics in Mizar\nPartial application of AI technology to mathematics Progress of GPT-f series models in automatic reasoning: some IMO difficulty problems can be done.\nRef. [1]Generative Language Modeling for Automated Theorem Proving [2]Mathematical Reasoning via Self-supervised Skip-tree Training [3]Proof Artifact Co-training for Theorem Proving with Language Models [4]Formal Mathematics Statement Curriculum Learning\nMachine learning guides human intuition and uses clustering algorithms to capture patterns in examples of knot theory and symmetric group theory, resulting in the discovery of two new theorems.\nRef. [1]Advancing mathematics by guiding human intuition with AI\nA neural network solver that can solve undergraduate mathematical problems by using Python\u0026rsquo;s symbolic computation package sumpy.\nRef. [1]A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More\nPossible technical paths I am currently working on a data set corresponding to Lean in natural language. After obtaining the data set, I will design the methods of automatic translation theorem and proof respectively. Translation of the theorem can generally be done using the transformer class model; The translation of proof should be a reinforcement learning task, which is not very clear for the time being.\nOr you can try an unsupervised approach. SeeUnsupervised Translation of Programming Languages\n","permalink":"https://subfish-zhou.github.io/posts/autoformalization/","summary":"new subject","title":"Automatic formalization of mathematics: a path to automatic reasoning"},{"content":"Introduction of Deep Learning for Automatic Theorem Proving Basic mathematics is finally waiting for a day when it can be computer-aided to massively increase productivity. In recent years, the field of automatic theorem proving (ATP) has generated an amazing amount of work through the introduction of deep learning and reinforcement learning, especially with the rapid progress of Large language models (LLM). Although we are still a little far away from having computers prove big propositions like the Riemann conjecture on their own, we can already talk predictably about how automatic theorem proving will help human mathematicians do their job. This paper pays tribute to the influential @Peng Keyao \u0026ldquo;Introduction to Computer-Aided Proof\u0026rdquo;. The formal mathematical language introduced in this paper is also the theoretical background of this paper. Modern ATP system is basically carried out on the basis of formal language. It is also shown as tactic in Lean or hammer in Isabelle, so it is suggested that readers who have not read this article should refer to it first.\nThis article focuses on the current progress in automatic theorem proving with deep learning and reinforcement learning (abbreviated DL+ATP below, but note that RL is also key). DL+ATP is developing rapidly, but I don\u0026rsquo;t want this article to be out of date, so I won\u0026rsquo;t cover too much technical content, and I will keep up with the field with another review that will be updated continuously. The purpose of this article is to present ideas, implications, clarify misconceptions, and answer doubts, with the aim of exposing this new, vibrant and significant field to enthusiasts and researchers in mathematics, computing, and beyond.\nThis article begins with the following questions (but the answers are intertwined, so the table of contents is not organized there) :\nWhy theorems can be proved automatically;\nHow DL+ATP works;\nWhat achievements have been made in the field of automatic proof;\nWhat mathematicians hope to gain from automatic proof;\nWhat are the effects of automatic theorem proving;\nBeginners how to get started with DL+ATP.\nHere are some of the results so far: As of the writing time of this paper on July 1, 2023.1, the most representative and influential DL+ATP work HyperTree Proof Search for Neural Theorem Proving (2022.5.23, see Meta Blog, paper, HTPS), automatically solved 10 International Mathematical Olympiades (IMO) problems, such as one of them:\n（IMO1964p1_2）7 can\u0026rsquo; t divide $2^n+1$ exactly.\nThe formal expression of this problem in Lean and the automatic proof process given by AI are as follows:\nimport data.nat.prime\rtheorem imo_1964_p1_2 (n : ℕ) :\r¬ 7 ∣ (2 ^ n + 1) :=\rbegin\rrw nat.dvd_iff_mod_eq_zero,\rrewrite [nat.add_mod, nat.mod_eq_of_lt],\robviously,\rapply nat.strong_induction_on n,\rinduction n,\r{\rintros n IH,\rcases n,\rnorm_num,\rcases n,\rnorm_num,\rrw [nat.succ_eq_add_one, pow_succ],\rrw [nat.succ_eq_add_one, pow_succ],\rinduction n,\rnorm_num,\rrw [nat.succ_eq_add_one, pow_succ],\rnorm_num [nat.mul_mod, ←mul_assoc],\rcontrapose! IH,\rrefine ⟨n_n, nat.lt_succ_iff.mpr _, IH⟩,\rexact nat.le_succ_of_le (nat.le_succ _),\r},\rexact n_ih,\rend This automatically generated proof (after a lot of simplification) translates into natural language roughly as follows:\nThe original problem is equivalent to $2^n+ the remainder of $1 divided by 7 is not equal to 0, so either $2^n divided by the remainder of 7 plus the remainder of 1 divided by 7 is not equal to 0, which is obvious, or $2^n divided by the remainder of 7 plus the remainder of 1 divided by 7 is less than 7. This proposition is proved by strong induction ∀(n :ℕ),(∀(m :ℕ),m \u0026lt; n → 2 ^ m % 7 + 1 \u0026lt; 7) → 2 ^ n % 7 + 1 \u0026lt; 7 inductive 1)∀ (m : ℕ), m \u0026lt; 0 → 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ 0 % 7 + 1 \u0026lt; 7 : obviously 2)∀ (m : ℕ), m \u0026lt; n + 1 → 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (n + 1) % 7 + 1 \u0026lt; 7 inductive 1.0) ∀ (m : ℕ), m \u0026lt; 1 → 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ 1 % 7 + 1 \u0026lt; 7 : obviously 1.1) ∀ (m : ℕ), m \u0026lt; n + 1 + 1 → 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (n + 1 + 1) % 7 + 1 \u0026lt; 7 inductive 1.1.0) ∀ (m : ℕ), m \u0026lt; 1 + 1 → 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (1 + 1)% 7 + 1 \u0026lt; 7 : obviously 1.1.1) ∀ (m : ℕ), m \u0026lt; n + 1 + 1 + 1→ 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (n + 1 + 1 + 1) % 7 + 1 \u0026lt; 7 So far, 2 ^ (n + 1 + 1 + 1) % 7 + 1 = 8 * 2 ^ n % 7 + 1 = (7 + 1) * 2 ^ n % 7 + 1 = 2 ^ n % 7 + 1, obviously.\nIf we look closely, we will find that this proof through extremely violent, with a strong mechanical wind means, in fact, also in disguised realization of the human proof \u0026ldquo;first observed\u0026rdquo; this \u0026ldquo;fantastic idea\u0026rdquo; (TL; DR, mainly reflected in the last step). It should be pointed out that this proof is not generated by a fixed algorithm of symbolism, such as SAT or Wu method, but is indeed actively searched out in the proof tree by thinking in a similar way to human mathematicians (algorithmic ideas will be briefly introduced in the following article). However, although the principle of this algorithm is close to human thinking, the result of this proof is really not like human writing, tedious and unelegant, mathematicians may feel that the rice bowl is still secure after reading. But this result also makes us reflect that we may indeed have come to the moment when we can discuss AI automatic theorem proving, although the \u0026ldquo;observation\u0026rdquo; of this problem is relatively simple, slightly trained students can master, but is the \u0026ldquo;art\u0026rdquo; of the theorem proving considered more profound really a kind of unattainable light of human wisdom, which is not allowed to be touched by AI?\nNote: You may have heard of LeanDojo (the work of the new intelligent Yuan scam), but the paper does not see that it is more powerful than Hypertree. We\u0026rsquo;ll talk about that later.\n自动证明是如何可能的 Automatic theorem proving is one of the earliest and most traditional problems in artificial intelligence, as far back as 1954, Martin Davis developed the first theorem proving program, proving that \u0026ldquo;the sum of two even numbers is even\u0026rdquo;; The Logic theorist program presented by Allen Newell and Herbert Simon at the Immortal Dartmouth Conference in 1956 proved more than half the theorems in Russell and Whitehead\u0026rsquo;s Principia Mathematica, and some were even more concise than the authors\u0026rsquo; proofs. Thus the school of symbols was founded. The influence of these early works on computer science is numerous, such as formal verification, expert systems and knowledge graphs are its descendants, and even the study of the SAT opened the discipline of computational complexity. The pinnacle of the symbolist line was the Vampire program, which is still being updated today and continues to play a role in areas such as formal verification. However, for the original goal of automatic proof of mathematical theorems, the symbolist route is inevitably limited by combinatorial explosion, so it can not be applied to the practice of mathematicians for a long time. In the statistical learning era, support vector machines and other models have been used to construct heuristics for proof search, but the results are not good. In recent years, with the addition of tools such as deep learning to automatic theorem proving, we finally have the hope that deep learning can develop heuristics good enough to overcome the combinatorial explosion and even achieve theorem proving algorithms that are close to the way humans think.\nBefore we look at how DL+ATP is possible, let\u0026rsquo;s look at how proof is structured as a computational task. There is no need to deal with the writing of successive calculus in proof theory, and we can ignore the concrete rules of calculus such as the resolution principle, and just from the most abstract task structure, we can think of the proof as a search tree. In analogy to the game of Go, each step has a number of points to choose, and eventually there will be several paths leading to victory, theorem proving can also be seen as in each propositional state (in the proof theory and ITP system, the states are context and goal) can apply a number of theorems or proof skills to the current state, so as to make it become the next state. And eventually there will be several paths leading to the completion of the proof. We use an example to show how search trees and the simplest heuristics work: the proof that \u0026ldquo;sqrt(2) is irrational\u0026rdquo;. If the reader is familiar with how ITP systems like Lean work, this example may be better understood.\nGiven this proposition, we have several options, for example, we can try induction and find that the proposition contains nothing that can be induced, so there is no way to perform induction. So maybe we can try proof by contradiction? Then we find that the proof by contradiction does change the state of the problem, and after restoring the definition back (that is, rewriting the definition in Lean) we get a new proposition \u0026ldquo;There are m,n, m,n mutual primes and sqrt(2)=m/n\u0026rdquo; and the goal becomes the derivation contradiction. Then there will be many choices, such as \u0026ldquo;both sides of the equation can be +1 at the same time\u0026rdquo; to apply, or \u0026ldquo;both sides of the equation can be multiplied by 2\u0026rdquo; to apply, or \u0026ldquo;both sides of the equation can be squared at the same time\u0026rdquo; to apply. If we don\u0026rsquo;t know the answer in advance, there are actually an infinite number of choices, that is, the search tree is infinitely wide. (In Lean, we have a lot of alternative tactics and theorems that can be rw) At this time, we have begun to see the \u0026ldquo;combinatorial explosion\u0026rdquo;, and the automatic proof algorithm cannot go through so many theorems, so some heuristics are needed to help select the next search branch. For example, you can use the number of symbols contained in the proposition to define a \u0026ldquo;proposition complexity\u0026rdquo;, and try to prevent the complexity of the proposition from rising when searching. In this way, the algorithm may give up trying to \u0026ldquo;+1 on both sides\u0026rdquo; because it makes the proposition more complicated. A less naive and more targeted heuristic at the moment might be \u0026ldquo;familiarity with symbols\u0026rdquo;, such as when the algorithm finds that it knows very little about sqrt, that only a few theorems can deal with it, and that more theorems can deal with integers and rational numbers, then it will tend to find a way to eliminate SQRT, at the moment using \u0026ldquo;squares of both sides\u0026rdquo;. Thus we (skipping a step) turn the original statement into \u0026ldquo;There are m,n, m,n mutual primes and 2=m²/n²\u0026rdquo;. Then the algorithm decides that it might not be that familiar with fractions, so it multiplies n² again, so the heuristic may stop there, and the algorithm will consider other search techniques.\nDue to Godel\u0026rsquo;s incompleteness theorem, Tarski\u0026rsquo;s first-order real numbers are decidable, but the super-exponential algorithm, and the NPC property of SAT, it can be argued that there is no \u0026ldquo;ultimate heuristic\u0026rdquo; that can guide all proofs, and Wu\u0026rsquo;s method that can completely solve elementary geometric problems is only a few special cases. In a general problem a heuristic can only be applied to a very narrow set of problem patterns. Theorem-prover in the era of symbolism is a big competition of heuristics, whose heuristic design is more delicate, who can better coordinate multiple heuristics, who can achieve better performance. But math problems are so varied that, even after decades of hard work, these provers are prohibitive about high school math, let alone keeping up with modern math.\nIn fact, other areas of AI have encountered similar situations, such as image recognition, which also relies on humans to manually design features, but performance on the Imagenet dataset has been hovering around 50%. In 2014, deep learning was born and instantly changed the face of the image recognition field, and now deep learning has also begun to emerge in the field of theorem proving.\nWhy is all this possible? You may feel that symbolic heuristic design may be in line with some human ideas in some cases, but it does not capture the most crucial part of human mathematical practice: humans have gone through a considerable amount of mathematical training, accumulated a lot of experience, and sometimes these experiences are difficult to describe, \u0026ldquo;I don\u0026rsquo;t know how I came up with it, but I just thought of it.\u0026rdquo; Deep learning can precisely allow the algorithm to accumulate experience, when it has done many many problems, when it encounters a new problem, it will recall whether it has seen a similar \u0026ldquo;problem type\u0026rdquo;, and find a similar practice with the training at that time.\nWe can use Monte Carlo search trees to describe a statistically enhanced theorem proving model. The Monte Carlo search tree gives a probability for each branch, such as 0.9 for \u0026ldquo;proof by contradiction\u0026rdquo;, 0.09 for \u0026ldquo;induction\u0026rdquo;, and 0.01 for some branches. The search is prioritized by probability, which avoids traversing all branches. In the symbolist era, in fact, the combined action of multiple heuristics would also use this model, but under a fixed combination of symbolic heuristics, the probability is usually fixed, but now we can adjust these probabilities through training (this is actually a reinforcement learning technique), For example, after searching for a branch with 0.9 probability, the algorithm finds that the descendants of this branch cannot successfully prove the theorem, indicating that the algorithm is wrong in estimating the success of this method, and then it will appropriately reduce this probability to avoid repeating the mistake (only reduce rather than completely kill it). The descendants of this branch may not be completely traversed and therefore it is not entirely certain that it is not possible). The HTPS algorithm introduced at the beginning of this article goes a step further on this idea, and it finds that people will have relatively fixed problem-solving routines when they encounter some problem types, that is, a series of theorem application steps may appear repeatedly, and only a few parameters may change according to specific circumstances. It then packages these routines into trees that become alternatives at each search step, eliminating the need to search one by one for theorems to be applied next. This \u0026ldquo;tree on tree\u0026rdquo; idea is the origin of \u0026ldquo;Hypertree\u0026rdquo;.\nThe probabilistic feedback mechanism of the Monte Carlo search tree alone is not enough, because the tree is too large (infinite), most of the solutions can not be found, so it is necessary to accumulate experience from training through deep neural networks, which are given a good enough initial probability in the Monte Carlo tree model. What deep learning does best is capture features, such as features of cats and dogs in images, or features common to English and Chinese. In the most abstract sense, analytical theorems are more likely to be proved analytically, and algebraic theorems are more likely to be solved with algebraic tools. Even if algebraic theorems are really proved analytically, we can also feel some \u0026ldquo;analytical flavor\u0026rdquo; in them. These are actually a kind of restriction on the direction of search. Deep neural networks can \u0026ldquo;accumulate experience\u0026rdquo; by discovering the features of proposition or proof process in the data, and adopting the corresponding proof process when encountering propositions with similar characteristics.\nFeatures in different task data often have their own uniqueness, and we can better extract them by designing networks with different structures accordingly. For example, we use convolutional neural networks to capture features in images. So what kind of deep neural network should we design to extract the features in the proof? Some earlier work had treated proofs as sequences or graphs, and accordingly used RNNS or GNNS to process them, but with little success. The recent success of large language models has led us to think of proofs as a language, and to use language models to capture features. The GPT-f family of work represented by HTPS uses a language model similar to GPT2, while the more recent LeanDojo uses a T5 model. The details of how these language models work and how different language models differ in proving theorems are outside the topic of this article. Readers are free to inquire for their own information, which will be covered in a technical overview later in this article.\nBut we also see that training theorem proving on LLM is not good enough. Part of LeanDojo\u0026rsquo;s work shows that GPT4 cannot prove complex mathematical theorems such as Stirling\u0026rsquo;s formula, even with sufficient prompt. The reason may be that there are too few theorem-proof data, and the most suitable method of LLM training has not been found, resulting in the lack of full use of LLM capabilities; It is also possible that the task of theorem proving itself has a richer complex and fine structure than language, and the violent aesthetics of LLM are not enough to conquer the arduous task of theorem proving. But while the task is daunting, it is not as mysterious as it was a few decades ago, and the mechanisms of proof and even reasoning have accumulated a wealth of knowledge, but there is still a lot of work to be done.\nThis section concludes with a response to some common questions:\nBecause of Godel\u0026rsquo;s incompleteness theorem (or for some other reason), a machine can never prove a mathematical theorem, because people are not bound by Godel\u0026rsquo;s theorem, so theorem proving can only be done by people.\nResponse: Godel actually has this view himself. But this view implies that man is an oracle Turing machine, that is, a Turing machine that can run for an infinite amount of time, and can travel through time to inform the present moment of the results of its operation after an infinite amount of time. According to the Church-Turing thesis, the Oracle is not a computational model that can be implemented in our universe. Corresponding to the task of proving theorems, this means that one can go through all the possibilities and find the right theorems to apply to without wasting time. It seems that human \u0026ldquo;inspiration\u0026rdquo; has a similar quality, but it actually means that one can find a proof without mathematical training, just by dreaming of a fairy. I prefer to believe that inspiration comes from some specific computational mechanism, even though it is currently only incompletely described by theories such as deep learning and reinforcement learning.\nBut people can write axiom systems such as ZFC, they can also know that these axiom systems are not complete and then add axioms, they also invented the Turing machine and know that the Turing machine cannot solve the halting problem, people can set the foundation for mathematical proofs and Turing machines, and the machine itself has no way to set the foundation for itself.\nResponse: This is to describe the level at which one can work on a metalinguistic level, that is, we study the properties of another language by virtue of language and only by virtue of language. A metalanguage is also a language, subject to the rules of computation, and presenting an axiomatic system and a halting problem is no different from other Turing-computable problems. Besides, people can\u0026rsquo;t solve the shutdown problem. There is no reason to think that Turing machines cannot observe themselves, just as modern computers can also check their own memory for errors, it is simply engineering.\nPresent and future of DL+ATP In December 2021, a paper titled \u0026ldquo;Advancing mathematics by guiding human intuition with AI\u0026rdquo; was published in Nature, It tells the story of Deepmind researchers who used traditional algorithms such as clustering to refine some laws in knot theory and representation theory, helping human mathematicians prove two new theorems. In June 2023, Tao evaluated how language models such as GPT4 helped him in his mathematical research, such as helping him quickly extract the content of other people\u0026rsquo;s papers. AI is popping up in and out of mathematics, helping mathematicians with all sorts of work, but AI writing its own mathematical proofs seems a long way off. On the one hand, mathematics itself is very complex, and on the other hand, it is limited by the lack of data. Humans do write many papers and textbooks, accumulate a large amount of mathematical corpus, and LLMS do train on mathematical corpus such as Wikipedia. If you ask ChatGPT about math, it can answer a lot of valuable information, but interestingly, when you ask it to prove a theorem it has never seen before, it will output sentences that look like math but are full of errors, as if a writer who doesn\u0026rsquo;t know math is trying to create lines for a mathematician character. Deep learning is good at capturing features, but it can\u0026rsquo;t use propositions and verify the correctness of propositions, so it can only \u0026ldquo;look\u0026rdquo; like mathematics rather than actually implement mathematics. There is actually no machine that can verify the correctness of natural language mathematical proofs, and sometimes not even humans themselves, because every now and then there are papers that no human being can understand. At present, only formal languages such as Metamath, Isabella, Coq, and Lean can be used as the basic data for AI to understand mathematics, because they can be compiled and run to verify correctness.\nIn 1973, Andrzej Trybulec developed Mizar, the first formal language specifically for mathematics, as well as an associated mathematics library, in order to verify the correctness of his doctoral thesis (although fifty years later, we have not been able to realize his ideal). This theorem library is still under maintenance and has accumulated 1w+ definitions 65k+ theorems. There are still ATP algorithms tested on Mizar. In recent years, type theory languages such as Coq and Lean have built larger theorem libraries, which have become more and more popular, and test question libraries such as MiniF2F (F2F is short for formal to formal) have been established to measure the performance of ATP algorithms. MiniF2F contains 488 questions described in four formal languages (Coq, Lean, Isabelle, Hol light, but Hol light only contains 330 questions) and natural language, collected from various mathematical competitions, including many questions from the famous mathematical Odyssey IMO. For example, the question shown at the top of the article. The researchers also launched the IMO Grand Challenge, hoping to bring AI to the IMO gold medal.\n(There is a hole to fill here, Mizar is very readable, very natural language like, and there is a recent rewritten version of Rust, I don\u0026rsquo;t know why it hasn\u0026rsquo;t caught on like Coq or Lean, wait for anyone who meets Mizar group to ask) The original motivation for mathematicians to develop formal languages was to test proofs programmatically and mechanically, but perhaps the most profound change this tool brought to the study of mathematics was that it clearly divided the study of mathematics into two parts: the form, the strict statement that defines the theorem and the calculus of the proof; And ideas, that is, why the definition theorem is written the way it is, why a proposition is used in a proof. Ideas are the most mysterious and confusing things in mathematics, excellent ideas are often named genius, and even regarded as miracles, ordinary people can only admire, let alone use scientific means to study and reproduce these ideas. Category theory, for example, may be one of those attempts that belongs to mathematicians. Now we have the tools of cognitive science and the theory of artificial intelligence, and as the first section shows, the laws of mathematical ideas are being revealed step by step. In order to more clearly demonstrate the dichotomy between form and idea in mathematics, I have been thinking of an experimental project to write a complete mathematics textbook in a formal language. What can be written in formal language is all formalized, and there must be something left that must be written in natural language, and these are the parts of the idea. These natural languages can be a great source of research for ideas.\nThe most pressing problem facing DL+ATP research is that mathematical propositions expressed in formal language are still a drop in the ocean compared to the entire mathematical edifion of mankind. Fortunately, the development of LLM technology has led to \u0026ldquo;pre-training\u0026rdquo; and \u0026ldquo;fine-tuning\u0026rdquo; tools that can circumvent the lack of data to some extent. The idea is that many of the abilities and knowledge reflected in the data are universal, such as reasoning ability; The ability required for a particular task may be specialized, such as the specific syntax of a formal language. If the LLM has a general knowledge of the language before learning the domain-specific task, it is much better than the traditional method of directly letting the AI do the specific task from random initialization. Specifically, ATP, natural language mathematics, and formal language mathematics express the same mathematical meaning despite their differences in rigor and grammar, so the LLM pre-trained on a large number of human mathematical languages, learned some mathematical knowledge first, and then specialized in fine-tuning the scarce formal languages, should have a much better effect than only learning on formal language data. The classic work of DL+ATP, GPT-f, is to pretrain GPT3 on the natural language data set CommonCrawl, 23GB of Github code, 10GB of Arxiv Math papers, and 2GB of Math StackExchange forum articles. The motivation for training on code is that code also reflects human reasoning, much like proof. However, the subsequent improvement work of GPT-f HTPS believes that it is better to only pre-train on Arxiv, and who can say the configuration of optimized performance in engineering. HTPS has achieved a pass rate of 58.6% on the verification set and 41.0% on the test set of Lean language MiniF2F, which is the best result of pure proof search relying only on formal language. More information on evaluating data sets and algorithm performance can be found on the PaperWithCode page, but the information on this page is incomplete and has not been maintained for a long time.\nNote 1: One of the drawbacks of fine-tuning is that it causes the LLM to focus only on domain-specific tasks and forget about general tasks, so it cannot be mathematically fine-tuned by tools such as ChatGPT to give it strong mathematical capabilities, which would make it lose other capabilities. Mathematical data sets were already part of ChatGPT\u0026rsquo;s pre-training, although the pre-training data did not give it enough mathematical power. Although LLMS can be pre-trained to learn mathematics beyond the available theorem proof data, the lack of theorem proof data is always a major hindrance to advancing the field. Readers who have the experience of using formal language will have a deep understanding that the process of mathematical proof in formal language is much more difficult than that of writing proof in natural language. Many details that we usually think are very ordinary and natural need to be completed, so the establishment of formal language theorem library needs a lot of human labor. If writing a formal language is such a hassle, why not let AI do it? The first attempt was made in 2018, followed by a report by Christian Szegedy in 2020, which formally raised the issue of autoformalization, the automatic translation of natural language mathematical propositions and proofs into formal language, like machine translation. The best performance of automated formalization tasks to date is also achieved by LLM, with a May 2022 test reporting that the best programming language model, Codex, could correctly translate 38 problems from natural language to Isabelle out of 150 problems extracted from the MATH dataset (which contains 12,500 natural language expressed math problems). It can be imagined that an algorithm that can translate two different mathematical languages well will inevitably involve an understanding of the nature of mathematical semantics, so the study of automatic formalization will also feed into the study of proof.\nIn terms of the concrete realization of the two problems, automatic formalization and ATP are also complementary tasks, because natural language proof is full of leaps, these natural omissions for humans can not be handled in the formal system, then ATP can be used to automatically complete. In turn, there is a clever practical idea that since formal language data is scarce, the search process for ATP can also be guided by natural language propositions. Representative work in this direction is the 2022 Draft, sketch, and prove: Guiding formal theorem provers with informal proofs (DSP), which uses Codex on Isabelle language to map natural language proofs into formal language proofs \u0026ldquo;draft\u0026rdquo;, that is, there are some leap-forward formal proofs. These hops are then automatically completed with Isabelle\u0026rsquo;s symbolic autoprover Sledgehammer (Isabelle\u0026rsquo;s symbolic autoprover is much stronger than Lean\u0026rsquo;s). The natural language proofs could have been written by a human expert, who achieved 39.3% test set accuracy, or they could have been produced by another large model of natural language mathematical questions, Minerva, which achieved 38.9% test set accuracy. If Minerva is used, then both the natural language template and the formal language proof are produced by the algorithm itself, which can also be seen as the algorithm itself doing the ATP task. the best subsequent results from this method came from Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving, which is an improvement of DSP, also achieves 45.5% accuracy of the test set on Isabelle, which is also the best result on MiniF2F dataset. However, it is based on a different formal language from HTPS, and relies on natural language and cannot be directly indexed.\nNote: One possible challenge to the DSP is that the Minerva training data contains mathematical text collected on the web page and the Arxiv text, which does not necessarily exclude the MiniF2F questions and is not \u0026ldquo;clean\u0026rdquo; enough, but the large language model can be regarded as a compression of the data, and generally cannot \u0026ldquo;recite\u0026rdquo; the solution, but can be regarded as a re-generated solution. However, Microsoft Copilot has done a job of reproducing irrelevant program comments, and the LLM\u0026rsquo;s internal data processing principles are still harder to understand. These results are far from ideal, but they have now translated some ATP tools that mathematicians may not think they can use. 22 years LEAN-GPTF as a proof tactic in Lean open test for a period of time, can prove some simple theorems, now closed test. HTPS has been integrated into Lean\u0026rsquo;s VSCode editor extension, and when you write a proof you click a button to get the next sentence of a dozen AI recommendations, most of which are wrong, but some are right. VSCode\u0026rsquo;s Copilot programming assistant can occasionally translate natural language math statements you write into Coq or Lean, and ChatGPT can occasionally write correct formal language proofs. The current situation looks bleak, but remember that this direction has only begun to develop in the last two or three years.\nIf you are willing to give a few more years of patience, what will DL+ATP bring to the world? The most immediate is to accelerate the development of formal mathematics. Introduction to Computer-Aided Proofs shows us a reliable and fair picture of mathematical research, and ATP can greatly reduce the cost of realizing this vision. We can feed a human math paper or a student\u0026rsquo;s answer to a test question into an automated formal algorithm, and then have it generate code to verify correctness. Alternatively, you can translate the code into human language to make it easier to read, or extract concise ideas from complex proofs to help you understand them. We may not be able to make ATP system prove some big theorems for a long time, but it is very likely to assist in the process of human thinking about mathematics, for example, an important possibility is the auxiliary retrieval of mathematical content: An idea came to my mind, and I wondered if anyone else had come up with a similar idea, maybe there was a precise theorem that expressed the idea in a way that traditional keyword matching searches could not find, but AI could understand the idea at an abstract semantic level to find relevant data.\nFinally, a little sci-fi fantasy. If one day mathematical AI can prove the Riemann conjecture, it is not difficult to imagine that AI can also solve scientific problems such as room-temperature superconductivity, controlled nuclear fusion, eliminate various diseases, and even solve all social production problems. Mathematics represents the peak of human abstract reasoning ability, and when AI also has abstract reasoning ability, it may be able to become the complete general artificial intelligence. What will society look like then?\nLearning Map If you are an enthusiast who wants to learn more about this field, with the goal of generally reading the leading papers in this direction, then you will need some introduction to deep learning and a preliminary understanding of LLM, at least familiar with the principles of the encoder-decoder architecture. It is more important to have experience with at least one formal language, and I recommend Lean here because modern DL+ATP is increasingly using Lean.\nIf you want to pursue research in this field\u0026hellip;\u0026hellip; Dissuade warning! You\u0026rsquo;re dealing with the most complex and comprehensive field in AI and the second most distant from money after people who do theory. (If any friend is not satisfied, please contact me to help ATP improve the ranking, thank x in advance)\nDSP instructor Albert Jiang pointed out that DL4Math people have a lot of money. HTPS writers Guillaume, Tim, Thibault, Marie-Anne went to mistral ai, Google\u0026rsquo;s Yuhuai Wu, Christian went to xai, jesse founded morph, stanislas founded dust, markus is starting a business. But I refuse to admit it until someone gives me money (x). The first is the need for more comprehensive knowledge of deep learning and reinforcement learning. LLM Needless to say, the main paradigm today is based on LLM, but CV knowledge is also helpful, because geometric propositions and the geometric intuitions of some propositions also need to be taken into account; Graph-based approaches have potential, so an understanding of GNN-related approaches can also be helpful; The latest work even uses the popular Diffusion method. In addition to these modern techniques, as seen before, DL and RL are still developed under the framework built in the pre-deep learning era, so it is important to understand the traditional methods. Outside of AI, you\u0026rsquo;ll be working on the basis of a type theory language, so you\u0026rsquo;ll need to know some \u0026ldquo;common sense\u0026rdquo;, such as mathematical logic and various type theories, functional programming, category theory, and so on. Some subproblems with a strong symbolic tradition will emphasize this knowledge more, but work like HTPS will use it as background knowledge. Overall, the road ahead for ATP is not clear, and the accumulated knowledge in this field is not deep enough to get started quickly, but researchers must have a fairly wide range of knowledge to find the next breakthrough, although they may not be directly applicable, but can provide more insight or at least not cause you to make a simple mistake.\nThe main event in the field of DL+ATP is the Artificial Intelligence and Theorem Proving conference AITP, and the Intelligent Computer Mathematics Conference CICM is also noteworthy. In addition, the MATH-AI workshop has been held in ICLR2021 and NeurIPS2022 for two sessions, and the third session will be held in NeurIPS2023 at the end of this year (2023.12.15/16). Welcome to your attention. At the NeurIPS conference, Albert Jiang, Kaiyu Yang and Emily First will also provide tutorial on machine learning theorem proving. Welcome to join us! In addition to conferences, if you\u0026rsquo;re in the Lean zulip community, check out stream Machine-Learning-for-Theorem Proving.\nFinally, post a list of papers for further study.\nA survey of deep learning for mathematical reasoning (2022). An overview is always the best place to start.\nHolophrasm: a neural automated theorem prover for higher-order logic (2016). DL+ATP started with quite advanced ideas, and there was even no transformer at that time. Unfortunately, the author has retired now.\nGenerative language modeling for automated theorem proving (2020). The famous GPT-f OpenAI is in with a lot of money.\nProof artifact co-training for theorem proving with language models (2021). Another classic work PACT, which uses a new training method and is more robust than GPT-f, is the main control group for future work.\nFormal mathematics statement curriculum learning (2022). The highlight is the use of curriculum learning, which is stronger than PACT.\nHypertree proof search for neural theorem proving (2022). The strongest HTPS, GPT-f scale +Holophrasm search algorithm. OpenAI\u0026rsquo;s big money turned around to hit ChatGPT, Meta took over OpenAI, but it\u0026rsquo;s not open source.\nDT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function (2023). The latest achievement, produced by Sun Yat-sen University, Peking University and Huawei. Slightly better than PACT.\nLeanDojo: Theorem Proving with Retrieval-Augmented Language Models (2023). I do not want to list it, but this article has been on the public number for a long time, in fact, the main work is to establish a data set. The structure of the model proposed in the paper is simple, and the Reprover model feels like a matching validation of the data set.\nThere are also Skip-tree Training, TacticToe, LISA and other equally wonderful jobs.\nThe above is the most basic ATP work using pure formal language, and there are more natural language components below.\nThor: Wielding hammers to integrate language models and automated theorem provers (2022) This is Thor, Go beyond PACT by having a language model that is not fine-tuned on a formal language dataset work with the formal language editor\u0026rsquo;s automatic search function.\nSolving quantitative reasoning problems with language models (2022). This is Minerva, natural language math, and even physics and chemistry.\nDraft, sketch, and prove: Guiding formal theorem provers with informal proofs (2022). This is DSP, using natural language proofs to guide the standard work of formal language proofs, going beyond Thor.\nDecomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving (2023). in this paper, the Subgoal method of reinforcement learning is introduced, and the optimal result on MiniF2F is achieved by using Diffusion on the graph to optimize in context learning.\nAutoformalization with large language models (2022). This article is enough for the direction of pure breed automatic formalization, and for early research you can check the Related Work section of this article.\nEvaluating Language Models for Mathematics through Interactions (2023). A detailed survey report on the mathematical ability of ChatGPT class large language models.\nThis list is extensive, interested friends can follow the guide in the above articles. There may be a lot of important work I can\u0026rsquo;t think of right now, please add in the comments section.\n","permalink":"https://subfish-zhou.github.io/posts/dlatp/","summary":"DL4ATP","title":"Introduction of Deep Learning for Automatic Theorem Proving"}]