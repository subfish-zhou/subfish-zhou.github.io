[{"content":"My Profile Hi I\u0026rsquo;m Subfish Zhou! Studying AI4Math. My interest in AI4MATH began during my sophomore year, when realized the potential and profound impact of using AI to solve mathematical problems. The AI4MATH technology contributes to our understanding of intelligence, and the technology can directly increase human knowledge of mathematics, physics, and other natural sciences. Imagine how the world would change if AI could help us solve Riemann Hypothesis or create the controlled fusion technology!\nMy work Article:\nIntroduction of Deep Learning for Automatic Theorem Proving\nTheorem proving in Lean4 (translation)\nMathematics in Lean4 (translation, in progress)\nReport:\nReview of DL4ATP (in progress). In Swarma Club.\nProgress in automatic theorem proving of LLM . In The First Type Theory Summer School.\nHow to think about a thinking machine. In Geek College.\nConsciousness, knowledge, ways of thinking, and general artificial intelligence . In Geek College.\n","permalink":"https://subfish-zhou.github.io/posts/myprofile/","summary":"Who is Subfish","title":"My Profile"},{"content":"Diffusion æœ¬æ–‡æ˜¯Googleå‘å¸ƒçš„Understanding Diffusion Models: A Unified Perspectiveä¸€æ–‡çš„ç¬”è®°ã€‚\nç”Ÿæˆæ¨¡å‹ç®€ä»‹ ç»™å®šæˆ‘ä»¬å…³æ³¨çš„æ•°æ®åˆ†å¸ƒä¸­çš„è§‚æµ‹æ ·æœ¬$x$ï¼Œç”Ÿæˆæ¨¡å‹çš„ç›®æ ‡æ˜¯å­¦ä¹ çœŸå®çš„æ•°æ®åˆ†å¸ƒ$p(x)$ã€‚è¿™æ ·ä¹‹åå°±å¯ä»¥ä»è¿‘ä¼¼æ¨¡å‹ä»»æ„ç”Ÿæˆæ–°çš„æ ·æœ¬ï¼Œä»¥åŠä½¿ç”¨è¿‘ä¼¼æ¨¡å‹æ¥è¯„ä¼°æ ·æœ¬æ•°æ®çš„æ¦‚ç‡ã€‚\nç›®å‰æœ‰å‡ ä¸ªè¾ƒé‡è¦çš„æ–¹å‘ï¼Œä¸‹é¢ç®€è¦æ¦‚æ‹¬ï¼š\nç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GANs)æ¨¡æ‹Ÿäº†ä¸€ä¸ªå¤æ‚åˆ†å¸ƒçš„æŠ½æ ·è¿‡ç¨‹ï¼Œå®ƒæ˜¯é€šè¿‡å¯¹æŠ—çš„æ–¹å¼å­¦ä¹ çš„ã€‚ â€œåŸºäºä¼¼ç„¶çš„æ¨¡å‹â€ï¼Œå°†é«˜ä¼¼ç„¶æ¦‚ç‡åˆ†é…ç»™è§‚æµ‹æ ·æœ¬ã€‚è¿™åŒ…æ‹¬è‡ªå›å½’æ¨¡å‹ã€æ­£è§„æµï¼ˆnormalizing flowsï¼‰å’Œå˜åˆ†è‡ªç¼–ç å™¨(VAE)ã€‚ åŸºäºèƒ½é‡çš„æ¨¡å‹ï¼Œåˆ†å¸ƒè¢«å­¦ä¹ ä¸ºä»»æ„å¯å˜çš„èƒ½é‡å‡½æ•°ï¼Œç„¶åè¢«å½’ä¸€åŒ–ã€‚ åŸºäºåˆ†æ•°çš„æ¨¡å‹ï¼Œä¸èƒ½é‡æ¨¡å‹å¾ˆç›¸å…³ï¼šå®ƒå°†åŸºäºèƒ½é‡çš„æ¨¡å‹çš„åˆ†æ•°ä½œä¸ºç¥ç»ç½‘ç»œæ¥å­¦ä¹ ã€‚ æœ¬æ–‡ä»‹ç»æ‰©æ•£æ¨¡å‹ï¼Œå®ƒæœ‰åŸºäºä¼¼ç„¶å’ŒåŸºäºåˆ†æ•°çš„è§£é‡Šã€‚ä¼šéå¸¸æ•°å­¦ï¼Œä¸è¿‡è¿™äº›æ¨å¯¼å¾ˆç»†èŠ‚ï¼Œåšå¥½å‡†å¤‡ï¼\nèƒŒæ™¯ï¼šELBOï¼ŒVAEå’Œåˆ†å±‚VAE è¯æ®ä¸‹ç•Œï¼ˆEvidence Lower Boundï¼‰ å¯¹äºå¾ˆå¤šæ¨¡å¼ä¸­çš„æ•°æ®ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ç”±æœªçŸ¥çš„éšå˜é‡$z$äº§ç”Ÿå‡ºçš„ã€‚æŠŠéšå˜é‡$z$å’Œæ•°æ®$x$å»ºæ¨¡æˆè”åˆæ¦‚ç‡åˆ†å¸ƒ$p(x,z)$ã€‚åŸºäºä¼¼ç„¶çš„æ¨¡å‹æœ€å¤§åŒ–æ‰€æœ‰è§‚æµ‹$x$çš„å¯èƒ½æ€§$p(x)$ï¼Œå¯ä»¥ç”¨ä¸¤ç§æ–¹æ³•æ“çºµè”åˆåˆ†å¸ƒæ’é™¤éšå˜é‡$z$æ¥æ¢å¤$p(x)$ï¼š $$p(x)=\\int p(x,z)dz\\tag{1}$$\n$$p(x)=\\frac{p(x,z)}{p(z|x)}\\tag{2}$$\nç›´æ¥è®¡ç®—ä»¥åŠæœ€å¤§åŒ–ä¼¼ç„¶å‡½æ•°$p(x)$æ˜¯å›°éš¾çš„ï¼Œå› ä¸ºç§¯åˆ†å’Œæ¡ä»¶æ¦‚ç‡çš„è®¡ç®—éƒ½æ¯”è¾ƒéš¾ã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªè¿‘ä¼¼çš„å˜åˆ†åˆ†å¸ƒ$q_{\\phi}(z|x)$æ¥ä¼°è®¡åéªŒæ¦‚ç‡$p(z|x)$ï¼Œç„¶åå»ä¼˜åŒ–æœ€å¤§ä¼¼ç„¶çš„å¯¹æ•°ï¼ˆè¿™é‡Œè¢«ç§°ä¸ºè¯æ®Evidenceï¼‰ï¼š\n$$ \\begin{aligned} \\log p(\\pmb{x}) \u0026amp;=\\log p(\\pmb{x}) \\int q_{\\phi}(\\pmb{z} | \\pmb{x}) d z \\ \u0026amp;=\\int q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})(\\log p(\\pmb{x})) d z \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}[\\log p(\\pmb{x})] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{p(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z}) q_{\\phi}(\\pmb{z} | \\pmb{x})}{p(\\pmb{z} | \\pmb{x}) q_{\\phi}(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})}\\right]+\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{q_{\\phi}(\\pmb{z} | \\pmb{x})}{p(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})}\\right]+D_{\\mathrm{KL}}\\left(q_{\\phi}(\\pmb{z} | \\pmb{x}) | p(\\pmb{z} | \\pmb{x})\\right) \\ \u0026amp; \\geq \\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\phi}(\\pmb{z} | \\pmb{x})}\\right] \\end{aligned} $$\nå› ä¸º$q_{\\phi}(z|x)$å’Œ$p(z|x)$çš„DLæ•£åº¦$D_{\\mathrm{KL}}\\left(q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x}) | p(\\pmb{z} | \\pmb{x})\\right) \\geq 0$ï¼Œæœ€åä¸€è¡Œå¾—åˆ°çš„æœŸæœ›è¢«ç§°ä¸ºè¯æ®ä¸‹ç•Œï¼ˆELBOï¼‰ï¼Œé€è¿‡ä¸ç­‰å¼ï¼Œæœ€å¤§åŒ–ELBOå°±ç­‰ä»·äºæœ€å¤§åŒ–ä¼¼ç„¶å’Œæœ€å°åŒ–DLæ•£åº¦ï¼ˆæ³¨æ„ï¼ŒELBOæ˜¯å…³äº$\\phi$çš„å‡½æ•°ï¼‰ã€‚\nå¦‚ä½•å®ç°ELBOçš„æœ€å¤§åŒ–ï¼Œäº¦å³å¦‚ä½•å®ç°ä¸€ä¸ªå¥½çš„$q_{\\phi}(z|x)$ï¼Œå°†æˆ‘ä»¬å¼•åˆ°äº†VAEä¹‹ä¸­ã€‚\nå˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVariational AutoEncoderï¼‰ è‡ªç¼–ç å™¨æ˜¯ä¸€ä¸ªå­¦ä¹ è¾“å…¥æ•°æ®$x$ï¼Œè·å¾—ä¸­é—´è¡¨ç¤º$z$ï¼Œç„¶åå†æ‹¿æ¥é¢„æµ‹$x$è‡ªå·±çš„æ¨¡å‹ã€‚$q_{\\phi}(z|x)$å°±æ˜¯ç¼–ç å™¨ï¼Œ$p_{\\theta}(x|z)$å°±æ˜¯è§£ç å™¨ã€‚è¯´å®ƒæ˜¯å˜åˆ†çš„ï¼Œæ˜¯å› ä¸º$q_{\\phi}(z|x)$ä»ä¸€æ—ç”±$\\phi$å‚æ•°åŒ–çš„éšåéªŒåˆ†å¸ƒä¸­ä¼˜åŒ–è€Œæ¥ã€‚\n","permalink":"https://subfish-zhou.github.io/posts/diffusion/diffusion/","summary":"summary","title":"ã€Noteã€‘Diffusion"},{"content":"General Visual Language Model æœ¬æ–‡è¯‘è‡ªLilian Wengçš„æ–‡ç« Generalized Visual Language Modelsï¼Œåœ¨æ­¤æ„Ÿè°¢å¥¹çš„è¾›å‹¤åˆ›ä½œï¼ŒçŒ®ä¸ŠèŠ±èŠ±ğŸŒ¸ã€‚ç”±äºç¿»è¯‘é‡è¾ƒå¤§ï¼Œæœºç¿»äº†éƒ¨åˆ†æ®µè½ï¼›è¿˜æœ‰ä¸€äº›æœ¯è¯­ä¸çŸ¥é“æ€ä¹ˆç¿»è¯‘æˆ–è€…ç¿»è¯‘å‡ºæ¥æ€ªæ€ªçš„ï¼Œéƒ½é™„ä¸Šè‹±æ–‡åŸæ–‡ã€‚è¯‘è€…èƒ½åŠ›æœ‰é™ï¼Œæ³è¯·è¯»è€…æµ·æ¶µã€‚\nå¤„ç†å›¾åƒç”Ÿæˆæ–‡æœ¬ï¼Œä¾‹å¦‚çœ‹å›¾è¯´è¯å’Œè§†è§‰QAï¼Œå·²ç»ç ”ç©¶äº†å¥½å¤šå¹´ã€‚ä¼ ç»Ÿä¸Šï¼Œè¿™ç§ç³»ç»Ÿä¾èµ–äºå¯¹è±¡æ£€æµ‹ç½‘ç»œä½œä¸ºè§†è§‰ç¼–ç å™¨æ¥æ•è·è§†è§‰ç‰¹å¾ï¼Œç„¶åé€šè¿‡æ–‡æœ¬è§£ç å™¨äº§ç”Ÿæ–‡æœ¬ã€‚æœ¬æ–‡æ€»ç»“äº†ç°ä»Šçš„ä¸å°‘è®ºæ–‡ï¼Œä¸“æ³¨äºè§£å†³è§†è§‰è¯­è¨€ä»»åŠ¡çš„ä¸€ç§æ–¹æ³•ï¼Œå³æ‰©å±•é¢„è®­ç»ƒçš„ä¸€èˆ¬è¯­è¨€æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿå¤„ç†è§†è§‰ä¿¡å·ã€‚\næˆ‘å°†è¿™ç§è§†è§‰è¯­è¨€æ¨¡å‹(VLMs)å¤§è‡´åˆ†ä¸ºå››ç±»:\nå°†å›¾åƒè½¬æ¢ä¸ºå¯ä¸æ–‡æœ¬tokenåµŒå…¥è”åˆè®­ç»ƒçš„åµŒå…¥ç‰¹å¾ã€‚ å­¦ä¹ å¥½çš„å›¾åƒåµŒå…¥ï¼Œå¯ä»¥ä½œä¸ºå›ºå®šçš„ã€é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹çš„å‰ç¼€ã€‚ ä½¿ç”¨ç‰¹åˆ«è®¾è®¡çš„äº¤å‰æ³¨æ„æœºåˆ¶å°†è§†è§‰ä¿¡æ¯èåˆåˆ°è¯­è¨€æ¨¡å‹çš„å„ä¸ªå±‚ä¸­ã€‚ ç»“åˆè§†è§‰å’Œè¯­è¨€æ¨¡å‹ï¼Œæ— éœ€ä»»ä½•è®­ç»ƒã€‚ å›¾åƒå’Œæ–‡æœ¬è”åˆè®­ç»ƒ å°†è§†è§‰ä¿¡æ¯èåˆåˆ°è¯­è¨€æ¨¡å‹ä¸­çš„ä¸€ç§ç›´æ¥æ–¹æ³•æ˜¯å°†å›¾åƒè§†ä¸ºæ™®é€šçš„æ–‡æœ¬tokenï¼Œå¹¶åœ¨æ–‡æœ¬å’Œå›¾åƒçš„è”åˆè¡¨ç¤ºåºåˆ—ä¸Šè®­ç»ƒæ¨¡å‹ã€‚å‡†ç¡®åœ°è¯´ï¼Œå›¾åƒè¢«åˆ†æˆè®¸å¤šå°å—(patch)ï¼Œæ¯ä¸ªå°å—è¢«è§†ä¸ºè¾“å…¥åºåˆ—ä¸­çš„ä¸€ä¸ªtokenã€‚\nVisualBERT(Li et al. 2019)å°†æ–‡æœ¬è¾“å…¥å’Œå›¾åƒåŒºåŸŸéƒ½è¾“å…¥åˆ°BERTä¸­ï¼Œè¿™æ ·å®ƒå°±èƒ½å¤Ÿé€šè¿‡è‡ªæ³¨æ„åŠ›å‘ç°å›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„å†…éƒ¨å¯¹é½ã€‚\nå›¾1ï¼šVisualBERTæ˜¯ç»“åˆæ–‡æœ¬å’Œå›¾åƒåµŒå…¥è¿›è¡Œè®­ç»ƒçš„ã€‚å›¾æºï¼š[(Li et al. 2019)](https://arxiv.org/abs/1908.03557)\nç±»ä¼¼äºBERTä¸­çš„æ–‡æœ¬åµŒå…¥ï¼ŒVisualBERTä¸­æ¯ä¸ªè§†è§‰åµŒå…¥ä¹Ÿæ˜¯ç”±ä¸‰ç§ç±»å‹çš„åµŒå…¥åŠ å’Œè€Œæˆçš„ï¼Œåˆ†åˆ«æ˜¯tokenizedç‰¹å¾$f_o$ï¼Œè¯†åˆ«åµŒå…¥(segment embedding) $f_s$å’Œä½ç½®åµŒå…¥$f_p$ã€‚å…·ä½“åœ°ï¼š\n$f_o$æ˜¯è§‰ç‰¹å¾å‘é‡ï¼Œç”±CNNåœ¨ä¸€ä¸ªè¾¹ç•ŒåŒºåŸŸä¸Šè®¡ç®—å¾—æ¥ï¼› $f_s$æ˜¯è¯†åˆ«åµŒå…¥ï¼Œç”¨äºè¯†åˆ«åµŒå…¥æ¥è‡ªè§†è§‰è¿˜æ˜¯æ–‡æœ¬ï¼› $f_p$æ˜¯ä½ç½®åµŒå…¥ï¼Œç”¨äºç»™è¾¹ç•ŒåŒºåŸŸæ’é¡ºåºã€‚ è¯¥æ¨¡å‹åœ¨MS COCOå›¾åƒæ ‡é¢˜æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œæ–‡æœ¬å’Œå›¾åƒéƒ½ä½œä¸ºè¾“å…¥æ¥é¢„æµ‹æ–‡æœ¬æè¿°(caption)ï¼Œä½¿ç”¨ä¸¤ä¸ªåŸºäºè§†è§‰çš„è¯­è¨€æ¨¡å‹ç›®æ ‡ï¼š\nå«å›¾åƒçš„æ©ç è¯­è¨€æ¨¡å‹MLMã€‚è¯¥æ¨¡å‹éœ€è¦é¢„æµ‹æ–‡æœ¬tokensä¸­çš„æ©ç ï¼Œè€Œå›¾åƒåµŒå…¥å§‹ç»ˆä¿æŒä¸æ©ç ã€‚ å¥å­-å›¾åƒé¢„æµ‹ã€‚æä¾›ä¸€å¼ å›¾ç‰‡å’Œä¸¤ä¸ªæè¿°å¥å­ï¼Œå…¶ä¸­ä¸€ä¸ªå¥å­æ˜¯å›¾ç‰‡ç›¸å…³çš„ï¼Œå¦ä¸€ä¸ªå¥å­æœ‰50%æ¦‚ç‡ç›¸å…³50%æ¦‚ç‡éšæœºã€‚æ¨¡å‹è¢«è¦æ±‚åŒºåˆ†è¿™ä¸¤ç§æƒ…å†µã€‚ æ ¹æ®æ¶ˆèå®éªŒï¼Œæœ€é‡è¦çš„é…ç½®æ˜¯å°†è§†è§‰ä¿¡æ¯æ—©æ—©èåˆåˆ°transformerå±‚ï¼Œå¹¶åœ¨COCO captionæ•°æ®é›†ä¸Šé¢„è®­ç»ƒæ¨¡å‹ã€‚ä»ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„BERTåˆå§‹åŒ–ä»¥åŠé‡‡ç”¨å¥å­-å›¾åƒé¢„æµ‹è®­ç»ƒç›®æ ‡ï¼Œå¸¦æ¥çš„å½±å“ç›¸å¯¹è¾ƒå°ã€‚\nå›¾2ï¼šVisualBERTåœ¨NLVRä¸Šçš„æ¶ˆèå®éªŒç»“æœã€‚å›¾æºï¼š[(Li et al. 2019)](https://arxiv.org/abs/1908.03557)\nVisualBERTåœ¨NLVRå’ŒFlickr30Kä¸Šçš„æ€§èƒ½ä¼˜äºSoTAï¼Œä½†åœ¨VQAä¸Šä»ä¸SoTAå­˜åœ¨ä¸€å®šçš„æ€§èƒ½å·®è·ã€‚\nSimVLM(Simple Visual Language Modelï¼›Wang et al. 2022)æ˜¯ä¸€ä¸ªç®€å•çš„å‰ç¼€è¯­è¨€æ¨¡å‹ï¼Œå…¶ä¸­å‰ç¼€åºåˆ—åƒBERTä¸€æ ·ç”¨åŒå‘æ³¨æ„åŠ›å¤„ç†ï¼Œä½†ä¸»è¾“å…¥åºåˆ—åƒGPTä¸€æ ·åªæœ‰å› æœæ³¨æ„åŠ›ã€‚å›¾åƒè¢«ç¼–ç ä¸ºå‰ç¼€tokenï¼Œè¿™æ ·æ¨¡å‹å°±å¯ä»¥å®Œå…¨é‡‡çº³è§†è§‰ä¿¡æ¯ï¼Œç„¶åä»¥è‡ªå›å½’çš„æ–¹å¼ç”Ÿæˆç›¸å…³çš„æ–‡æœ¬ã€‚\nå—ViTå’ŒCoAtNetçš„å¯å‘ï¼ŒSimVLMå°†å›¾åƒåˆ†å‰²æˆæ›´å°çš„å—ï¼Œå¹¶å±•æˆä¸€ç»´å°å—åºåˆ—ã€‚ä»–ä»¬ä½¿ç”¨ç”±ResNetçš„å‰3ä¸ªå—ç»„æˆçš„å·ç§¯æ­¥éª¤æ¥æå–åŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å°å—ï¼Œè¿™ç§è®¾ç½®è¢«å‘ç°æ¯”å¹³å‡¡çš„çº¿æ€§æŠ•å½±æ›´å¥½ã€‚\nå›¾3ï¼šSimVLMçš„è®­ç»ƒæ¶æ„ï¼Œå…¶ä¸­å›¾åƒå°å—ç”±äº¤å‰æ³¨æ„åŠ›ç¼–ç å™¨å¤„ç†ï¼Œæ–‡æœ¬è§£ç å™¨å…·æœ‰å› æœæ³¨æ„åŠ›ã€‚å›¾æºï¼š[Wang et al. 2022](https://arxiv.org/abs/2108.10904)\nSimVLMçš„è®­ç»ƒæ•°æ®åŒ…æ‹¬æ¥è‡ªALIGN(Jia et al. 2021)çš„å¤§é‡å›¾åƒ-æ–‡æœ¬å¯¹å’Œæ¥è‡ªC4æ•°æ®é›†(Raffel et al. 2019)çš„çº¯æ–‡æœ¬æ•°æ®ã€‚ä»–ä»¬åœ¨æ¯ä¸ªå°æ‰¹é‡(batch)ä¸­æ··åˆäº†ä¸¤ä¸ªé¢„è®­ç»ƒæ•°æ®é›†ï¼ŒåŒ…å«4096ä¸ªå›¾åƒ-æ–‡æœ¬å¯¹(ALIGN)å’Œ512ä¸ªçº¯æ–‡æœ¬æ–‡æ¡£(C4)ã€‚\næ ¹æ®æ¶ˆèå®éªŒï¼ŒåŒæ—¶æ‹¥æœ‰å›¾åƒ-æ–‡æœ¬å’Œçº¯æ–‡æœ¬æ•°æ®è¿›è¡Œè®­ç»ƒæ˜¯å¾ˆé‡è¦çš„ã€‚å‰ç¼€è¯­è¨€æ¨¡å‹ç›®æ ‡ä¼˜äºspan corruptionå’Œæ™®é€šè¯­è¨€æ¨¡å‹ã€‚\nå›¾4ï¼šSimVLMåœ¨VQAä¸Šçš„æ¶ˆèå®éªŒç»“æœã€‚å›¾æº:[(Wang et al. 2022)](https://arxiv.org/abs/2108.10904)\n","permalink":"https://subfish-zhou.github.io/posts/generalized_visual_lm/","summary":"summary","title":"ã€Translateã€‘General Visual Language Model"},{"content":"Automatic formalization of mathematics: a path to automatic reasoning Therefore it is natural to ask: Will we ever arrive at the point where an AI agent can learn to do reasoning as well as the best humans in the world in most established domains of mathematics. \u0026mdash;- Christian Szegedy\nThis paper refers to A Promising Path Towards Autoformalization and General Artificial Intelligence\nIt is well known that deep learning is not very good at reasoning, and it is natural for us to think that we can test reasoning AI mathematically. But before entering into the problem, there are two obvious difficulties: (1) humans express mathematical knowledge in natural language, and the fuzziness of natural language makes it difficult for computers to understand and verify it in a mechanized way; (2) Human mathematical knowledge is scattered in a vast number of papers, and it is difficult to generate effective data sets for system training.\nAt the same time, the development of human basic mathematics has also encountered an obvious bottleneck, the complexity of human mathematical papers is getting higher and higher, resulting in the difficulty of verifying their correctness, often a paper review time is extremely long or even unable to review the situation.\nIn response to these problems, some mathematicians have developed formal mathematical languages to represent mathematical knowledge, and these formal languages represent propositions and proof processes that can be verified by fixed procedures. These mathematicians hope to verify the correctness of human mathematical knowledge and promote the development of human mathematics through computers, but the workload of translating human mathematics into formal language is very huge, and the content of the mathematical theorem library of the main formal language is very limited, and even cannot completely include the content of undergraduate mathematics.\nTherefore, the need to develop a mathematical automatic formalization algorithm is particularly urgent, on the one hand, it can provide a large number of clear mathematical knowledge base for mathematical reasoning artificial intelligence, on the other hand, it can verify the correctness of human mathematical knowledge on a large scale.\nç®€å² The idea of automatic formalization was first proposed by John McCarthy in 1961. Another early attempt was Donald Simons\u0026rsquo; doctoral thesis in 1990. The first thorough study was carried out in Claus Zinn\u0026rsquo;s PhD thesis in 2004. None of this work has produced even a partially practical solution.\nJosef Urban began working on the subject in the early 2000s. He designed the first large-scale benchmark for large-scale theoretical reasoning, motivated by the belief that reasoning in a large knowledge base of mathematical facts is a key component of any automated formalization system. In 2007, he published the groundbreaking MaLARea system for large-scale theoretical reasoning. Since then, he and Cezary Kaliszyk have been spearheading research into inference and automatic formatting of large theories.\nRef. [1]First experiments with neural translation of informal to formal mathematics [2]Exploration of neural machine translation in autoformalization of mathematics in Mizar\nPartial application of AI technology to mathematics Progress of GPT-f series models in automatic reasoning: some IMO difficulty problems can be done.\nRef. [1]Generative Language Modeling for Automated Theorem Proving [2]Mathematical Reasoning via Self-supervised Skip-tree Training [3]Proof Artifact Co-training for Theorem Proving with Language Models [4]Formal Mathematics Statement Curriculum Learning\nMachine learning guides human intuition and uses clustering algorithms to capture patterns in examples of knot theory and symmetric group theory, resulting in the discovery of two new theorems.\nRef. [1]Advancing mathematics by guiding human intuition with AI\nA neural network solver that can solve undergraduate mathematical problems by using Python\u0026rsquo;s symbolic computation package sumpy.\nRef. [1]A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More\nPossible technical paths I am currently working on a data set corresponding to Lean in natural language. After obtaining the data set, I will design the methods of automatic translation theorem and proof respectively. Translation of the theorem can generally be done using the transformer class model; The translation of proof should be a reinforcement learning task, which is not very clear for the time being.\nOr you can try an unsupervised approach. SeeUnsupervised Translation of Programming Languages\n","permalink":"https://subfish-zhou.github.io/posts/autoformalization/","summary":"new subject","title":"Automatic formalization of mathematics: a path to automatic reasoning"},{"content":"Introduction of Deep Learning for Automatic Theorem Proving Basic mathematics is finally waiting for a day when it can be computer-aided to massively increase productivity. In recent years, the field of automatic theorem proving (ATP) has generated an amazing amount of work through the introduction of deep learning and reinforcement learning, especially with the rapid progress of Large language models (LLM). Although we are still a little far away from having computers prove big propositions like the Riemann conjecture on their own, we can already talk predictably about how automatic theorem proving will help human mathematicians do their job. This paper pays tribute to the influential @Peng Keyao \u0026ldquo;Introduction to Computer-Aided Proof\u0026rdquo;. The formal mathematical language introduced in this paper is also the theoretical background of this paper. Modern ATP system is basically carried out on the basis of formal language. It is also shown as tactic in Lean or hammer in Isabelle, so it is suggested that readers who have not read this article should refer to it first.\nThis article focuses on the current progress in automatic theorem proving with deep learning and reinforcement learning (abbreviated DL+ATP below, but note that RL is also key). DL+ATP is developing rapidly, but I don\u0026rsquo;t want this article to be out of date, so I won\u0026rsquo;t cover too much technical content, and I will keep up with the field with another review that will be updated continuously. The purpose of this article is to present ideas, implications, clarify misconceptions, and answer doubts, with the aim of exposing this new, vibrant and significant field to enthusiasts and researchers in mathematics, computing, and beyond.\nThis article begins with the following questions (but the answers are intertwined, so the table of contents is not organized there) :\nWhy theorems can be proved automatically;\nHow DL+ATP works;\nWhat achievements have been made in the field of automatic proof;\nWhat mathematicians hope to gain from automatic proof;\nWhat are the effects of automatic theorem proving;\nBeginners how to get started with DL+ATP.\nHere are some of the results so far: As of the writing time of this paper on July 1, 2023.1, the most representative and influential DL+ATP work HyperTree Proof Search for Neural Theorem Proving (2022.5.23, see Meta Blog, paper, HTPS), automatically solved 10 International Mathematical Olympiades (IMO) problems, such as one of them:\nï¼ˆIMO1964p1_2ï¼‰7 can\u0026rsquo; t divide $2^n+1$ exactly.\nThe formal expression of this problem in Lean and the automatic proof process given by AI are as follows:\nimport data.nat.prime\rtheorem imo_1964_p1_2 (n : â„•) :\rÂ¬ 7 âˆ£ (2 ^ n + 1) :=\rbegin\rrw nat.dvd_iff_mod_eq_zero,\rrewrite [nat.add_mod, nat.mod_eq_of_lt],\robviously,\rapply nat.strong_induction_on n,\rinduction n,\r{\rintros n IH,\rcases n,\rnorm_num,\rcases n,\rnorm_num,\rrw [nat.succ_eq_add_one, pow_succ],\rrw [nat.succ_eq_add_one, pow_succ],\rinduction n,\rnorm_num,\rrw [nat.succ_eq_add_one, pow_succ],\rnorm_num [nat.mul_mod, â†mul_assoc],\rcontrapose! IH,\rrefine âŸ¨n_n, nat.lt_succ_iff.mpr _, IHâŸ©,\rexact nat.le_succ_of_le (nat.le_succ _),\r},\rexact n_ih,\rend This automatically generated proof (after a lot of simplification) translates into natural language roughly as follows:\nThe original problem is equivalent to $2^n+ the remainder of $1 divided by 7 is not equal to 0, so either $2^n divided by the remainder of 7 plus the remainder of 1 divided by 7 is not equal to 0, which is obvious, or $2^n divided by the remainder of 7 plus the remainder of 1 divided by 7 is less than 7. This proposition is proved by strong induction âˆ€(n :â„•),(âˆ€(m :â„•),m \u0026lt; n â†’ 2 ^ m % 7 + 1 \u0026lt; 7) â†’ 2 ^ n % 7 + 1 \u0026lt; 7 inductive 1)âˆ€ (m : â„•), m \u0026lt; 0 â†’ 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ 0 % 7 + 1 \u0026lt; 7 : obviously 2)âˆ€ (m : â„•), m \u0026lt; n + 1 â†’ 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (n + 1) % 7 + 1 \u0026lt; 7 inductive 1.0) âˆ€ (m : â„•), m \u0026lt; 1 â†’ 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ 1 % 7 + 1 \u0026lt; 7 : obviously 1.1) âˆ€ (m : â„•), m \u0026lt; n + 1 + 1 â†’ 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (n + 1 + 1) % 7 + 1 \u0026lt; 7 inductive 1.1.0) âˆ€ (m : â„•), m \u0026lt; 1 + 1 â†’ 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (1 + 1)% 7 + 1 \u0026lt; 7 : obviously 1.1.1) âˆ€ (m : â„•), m \u0026lt; n + 1 + 1 + 1â†’ 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (n + 1 + 1 + 1) % 7 + 1 \u0026lt; 7 So far, 2 ^ (n + 1 + 1 + 1) % 7 + 1 = 8 * 2 ^ n % 7 + 1 = (7 + 1) * 2 ^ n % 7 + 1 = 2 ^ n % 7 + 1, obviously.\nIf we look closely, we will find that this proof through extremely violent, with a strong mechanical wind means, in fact, also in disguised realization of the human proof \u0026ldquo;first observed\u0026rdquo; this \u0026ldquo;fantastic idea\u0026rdquo; (TL; DR, mainly reflected in the last step). It should be pointed out that this proof is not generated by a fixed algorithm of symbolism, such as SAT or Wu method, but is indeed actively searched out in the proof tree by thinking in a similar way to human mathematicians (algorithmic ideas will be briefly introduced in the following article). However, although the principle of this algorithm is close to human thinking, the result of this proof is really not like human writing, tedious and unelegant, mathematicians may feel that the rice bowl is still secure after reading. But this result also makes us reflect that we may indeed have come to the moment when we can discuss AI automatic theorem proving, although the \u0026ldquo;observation\u0026rdquo; of this problem is relatively simple, slightly trained students can master, but is the \u0026ldquo;art\u0026rdquo; of the theorem proving considered more profound really a kind of unattainable light of human wisdom, which is not allowed to be touched by AI?\nNote: You may have heard of LeanDojo (the work of the new intelligent Yuan scam), but the paper does not see that it is more powerful than Hypertree. We\u0026rsquo;ll talk about that later.\nè‡ªåŠ¨è¯æ˜æ˜¯å¦‚ä½•å¯èƒ½çš„ Automatic theorem proving is one of the earliest and most traditional problems in artificial intelligence, as far back as 1954, Martin Davis developed the first theorem proving program, proving that \u0026ldquo;the sum of two even numbers is even\u0026rdquo;; The Logic theorist program presented by Allen Newell and Herbert Simon at the Immortal Dartmouth Conference in 1956 proved more than half the theorems in Russell and Whitehead\u0026rsquo;s Principia Mathematica, and some were even more concise than the authors\u0026rsquo; proofs. Thus the school of symbols was founded. The influence of these early works on computer science is numerous, such as formal verification, expert systems and knowledge graphs are its descendants, and even the study of the SAT opened the discipline of computational complexity. The pinnacle of the symbolist line was the Vampire program, which is still being updated today and continues to play a role in areas such as formal verification. However, for the original goal of automatic proof of mathematical theorems, the symbolist route is inevitably limited by combinatorial explosion, so it can not be applied to the practice of mathematicians for a long time. In the statistical learning era, support vector machines and other models have been used to construct heuristics for proof search, but the results are not good. In recent years, with the addition of tools such as deep learning to automatic theorem proving, we finally have the hope that deep learning can develop heuristics good enough to overcome the combinatorial explosion and even achieve theorem proving algorithms that are close to the way humans think.\nBefore we look at how DL+ATP is possible, let\u0026rsquo;s look at how proof is structured as a computational task. There is no need to deal with the writing of successive calculus in proof theory, and we can ignore the concrete rules of calculus such as the resolution principle, and just from the most abstract task structure, we can think of the proof as a search tree. In analogy to the game of Go, each step has a number of points to choose, and eventually there will be several paths leading to victory, theorem proving can also be seen as in each propositional state (in the proof theory and ITP system, the states are context and goal) can apply a number of theorems or proof skills to the current state, so as to make it become the next state. And eventually there will be several paths leading to the completion of the proof. We use an example to show how search trees and the simplest heuristics work: the proof that \u0026ldquo;sqrt(2) is irrational\u0026rdquo;. If the reader is familiar with how ITP systems like Lean work, this example may be better understood.\nGiven this proposition, we have several options, for example, we can try induction and find that the proposition contains nothing that can be induced, so there is no way to perform induction. So maybe we can try proof by contradiction? Then we find that the proof by contradiction does change the state of the problem, and after restoring the definition back (that is, rewriting the definition in Lean) we get a new proposition \u0026ldquo;There are m,n, m,n mutual primes and sqrt(2)=m/n\u0026rdquo; and the goal becomes the derivation contradiction. Then there will be many choices, such as \u0026ldquo;both sides of the equation can be +1 at the same time\u0026rdquo; to apply, or \u0026ldquo;both sides of the equation can be multiplied by 2\u0026rdquo; to apply, or \u0026ldquo;both sides of the equation can be squared at the same time\u0026rdquo; to apply. If we don\u0026rsquo;t know the answer in advance, there are actually an infinite number of choices, that is, the search tree is infinitely wide. (In Lean, we have a lot of alternative tactics and theorems that can be rw) At this time, we have begun to see the \u0026ldquo;combinatorial explosion\u0026rdquo;, and the automatic proof algorithm cannot go through so many theorems, so some heuristics are needed to help select the next search branch. For example, you can use the number of symbols contained in the proposition to define a \u0026ldquo;proposition complexity\u0026rdquo;, and try to prevent the complexity of the proposition from rising when searching. In this way, the algorithm may give up trying to \u0026ldquo;+1 on both sides\u0026rdquo; because it makes the proposition more complicated. A less naive and more targeted heuristic at the moment might be \u0026ldquo;familiarity with symbols\u0026rdquo;, such as when the algorithm finds that it knows very little about sqrt, that only a few theorems can deal with it, and that more theorems can deal with integers and rational numbers, then it will tend to find a way to eliminate SQRT, at the moment using \u0026ldquo;squares of both sides\u0026rdquo;. Thus we (skipping a step) turn the original statement into \u0026ldquo;There are m,n, m,n mutual primes and 2=mÂ²/nÂ²\u0026rdquo;. Then the algorithm decides that it might not be that familiar with fractions, so it multiplies nÂ² again, so the heuristic may stop there, and the algorithm will consider other search techniques.\nDue to Godel\u0026rsquo;s incompleteness theorem, Tarski\u0026rsquo;s first-order real numbers are decidable, but the super-exponential algorithm, and the NPC property of SAT, it can be argued that there is no \u0026ldquo;ultimate heuristic\u0026rdquo; that can guide all proofs, and Wu\u0026rsquo;s method that can completely solve elementary geometric problems is only a few special cases. In a general problem a heuristic can only be applied to a very narrow set of problem patterns. Theorem-prover in the era of symbolism is a big competition of heuristics, whose heuristic design is more delicate, who can better coordinate multiple heuristics, who can achieve better performance. But math problems are so varied that, even after decades of hard work, these provers are prohibitive about high school math, let alone keeping up with modern math.\nIn fact, other areas of AI have encountered similar situations, such as image recognition, which also relies on humans to manually design features, but performance on the Imagenet dataset has been hovering around 50%. In 2014, deep learning was born and instantly changed the face of the image recognition field, and now deep learning has also begun to emerge in the field of theorem proving.\nWhy is all this possible? You may feel that symbolic heuristic design may be in line with some human ideas in some cases, but it does not capture the most crucial part of human mathematical practice: humans have gone through a considerable amount of mathematical training, accumulated a lot of experience, and sometimes these experiences are difficult to describe, \u0026ldquo;I don\u0026rsquo;t know how I came up with it, but I just thought of it.\u0026rdquo; Deep learning can precisely allow the algorithm to accumulate experience, when it has done many many problems, when it encounters a new problem, it will recall whether it has seen a similar \u0026ldquo;problem type\u0026rdquo;, and find a similar practice with the training at that time.\nWe can use Monte Carlo search trees to describe a statistically enhanced theorem proving model. The Monte Carlo search tree gives a probability for each branch, such as 0.9 for \u0026ldquo;proof by contradiction\u0026rdquo;, 0.09 for \u0026ldquo;induction\u0026rdquo;, and 0.01 for some branches. The search is prioritized by probability, which avoids traversing all branches. In the symbolist era, in fact, the combined action of multiple heuristics would also use this model, but under a fixed combination of symbolic heuristics, the probability is usually fixed, but now we can adjust these probabilities through training (this is actually a reinforcement learning technique), For example, after searching for a branch with 0.9 probability, the algorithm finds that the descendants of this branch cannot successfully prove the theorem, indicating that the algorithm is wrong in estimating the success of this method, and then it will appropriately reduce this probability to avoid repeating the mistake (only reduce rather than completely kill it). The descendants of this branch may not be completely traversed and therefore it is not entirely certain that it is not possible). The HTPS algorithm introduced at the beginning of this article goes a step further on this idea, and it finds that people will have relatively fixed problem-solving routines when they encounter some problem types, that is, a series of theorem application steps may appear repeatedly, and only a few parameters may change according to specific circumstances. It then packages these routines into trees that become alternatives at each search step, eliminating the need to search one by one for theorems to be applied next. This \u0026ldquo;tree on tree\u0026rdquo; idea is the origin of \u0026ldquo;Hypertree\u0026rdquo;.\nThe probabilistic feedback mechanism of the Monte Carlo search tree alone is not enough, because the tree is too large (infinite), most of the solutions can not be found, so it is necessary to accumulate experience from training through deep neural networks, which are given a good enough initial probability in the Monte Carlo tree model. What deep learning does best is capture features, such as features of cats and dogs in images, or features common to English and Chinese. In the most abstract sense, analytical theorems are more likely to be proved analytically, and algebraic theorems are more likely to be solved with algebraic tools. Even if algebraic theorems are really proved analytically, we can also feel some \u0026ldquo;analytical flavor\u0026rdquo; in them. These are actually a kind of restriction on the direction of search. Deep neural networks can \u0026ldquo;accumulate experience\u0026rdquo; by discovering the features of proposition or proof process in the data, and adopting the corresponding proof process when encountering propositions with similar characteristics.\nFeatures in different task data often have their own uniqueness, and we can better extract them by designing networks with different structures accordingly. For example, we use convolutional neural networks to capture features in images. So what kind of deep neural network should we design to extract the features in the proof? Some earlier work had treated proofs as sequences or graphs, and accordingly used RNNS or GNNS to process them, but with little success. The recent success of large language models has led us to think of proofs as a language, and to use language models to capture features. The GPT-f family of work represented by HTPS uses a language model similar to GPT2, while the more recent LeanDojo uses a T5 model. The details of how these language models work and how different language models differ in proving theorems are outside the topic of this article. Readers are free to inquire for their own information, which will be covered in a technical overview later in this article.\nBut we also see that training theorem proving on LLM is not good enough. Part of LeanDojo\u0026rsquo;s work shows that GPT4 cannot prove complex mathematical theorems such as Stirling\u0026rsquo;s formula, even with sufficient prompt. The reason may be that there are too few theorem-proof data, and the most suitable method of LLM training has not been found, resulting in the lack of full use of LLM capabilities; It is also possible that the task of theorem proving itself has a richer complex and fine structure than language, and the violent aesthetics of LLM are not enough to conquer the arduous task of theorem proving. But while the task is daunting, it is not as mysterious as it was a few decades ago, and the mechanisms of proof and even reasoning have accumulated a wealth of knowledge, but there is still a lot of work to be done.\nThis section concludes with a response to some common questions:\nBecause of Godel\u0026rsquo;s incompleteness theorem (or for some other reason), a machine can never prove a mathematical theorem, because people are not bound by Godel\u0026rsquo;s theorem, so theorem proving can only be done by people.\nResponse: Godel actually has this view himself. But this view implies that man is an oracle Turing machine, that is, a Turing machine that can run for an infinite amount of time, and can travel through time to inform the present moment of the results of its operation after an infinite amount of time. According to the Church-Turing thesis, the Oracle is not a computational model that can be implemented in our universe. Corresponding to the task of proving theorems, this means that one can go through all the possibilities and find the right theorems to apply to without wasting time. It seems that human \u0026ldquo;inspiration\u0026rdquo; has a similar quality, but it actually means that one can find a proof without mathematical training, just by dreaming of a fairy. I prefer to believe that inspiration comes from some specific computational mechanism, even though it is currently only incompletely described by theories such as deep learning and reinforcement learning.\nBut people can write axiom systems such as ZFC, they can also know that these axiom systems are not complete and then add axioms, they also invented the Turing machine and know that the Turing machine cannot solve the halting problem, people can set the foundation for mathematical proofs and Turing machines, and the machine itself has no way to set the foundation for itself.\nResponse: This is to describe the level at which one can work on a metalinguistic level, that is, we study the properties of another language by virtue of language and only by virtue of language. A metalanguage is also a language, subject to the rules of computation, and presenting an axiomatic system and a halting problem is no different from other Turing-computable problems. Besides, people can\u0026rsquo;t solve the shutdown problem. There is no reason to think that Turing machines cannot observe themselves, just as modern computers can also check their own memory for errors, it is simply engineering.\nPresent and future of DL+ATP In December 2021, a paper titled \u0026ldquo;Advancing mathematics by guiding human intuition with AI\u0026rdquo; was published in Nature, It tells the story of Deepmind researchers who used traditional algorithms such as clustering to refine some laws in knot theory and representation theory, helping human mathematicians prove two new theorems. In June 2023, Tao evaluated how language models such as GPT4 helped him in his mathematical research, such as helping him quickly extract the content of other people\u0026rsquo;s papers. AI is popping up in and out of mathematics, helping mathematicians with all sorts of work, but AI writing its own mathematical proofs seems a long way off. On the one hand, mathematics itself is very complex, and on the other hand, it is limited by the lack of data. Humans do write many papers and textbooks, accumulate a large amount of mathematical corpus, and LLMS do train on mathematical corpus such as Wikipedia. If you ask ChatGPT about math, it can answer a lot of valuable information, but interestingly, when you ask it to prove a theorem it has never seen before, it will output sentences that look like math but are full of errors, as if a writer who doesn\u0026rsquo;t know math is trying to create lines for a mathematician character. Deep learning is good at capturing features, but it can\u0026rsquo;t use propositions and verify the correctness of propositions, so it can only \u0026ldquo;look\u0026rdquo; like mathematics rather than actually implement mathematics. There is actually no machine that can verify the correctness of natural language mathematical proofs, and sometimes not even humans themselves, because every now and then there are papers that no human being can understand. At present, only formal languages such as Metamath, Isabella, Coq, and Lean can be used as the basic data for AI to understand mathematics, because they can be compiled and run to verify correctness.\nIn 1973, Andrzej Trybulec developed Mizar, the first formal language specifically for mathematics, as well as an associated mathematics library, in order to verify the correctness of his doctoral thesis (although fifty years later, we have not been able to realize his ideal). This theorem library is still under maintenance and has accumulated 1w+ definitions 65k+ theorems. There are still ATP algorithms tested on Mizar. In recent years, type theory languages such as Coq and Lean have built larger theorem libraries, which have become more and more popular, and test question libraries such as MiniF2F (F2F is short for formal to formal) have been established to measure the performance of ATP algorithms. MiniF2F contains 488 questions described in four formal languages (Coq, Lean, Isabelle, Hol light, but Hol light only contains 330 questions) and natural language, collected from various mathematical competitions, including many questions from the famous mathematical Odyssey IMO. For example, the question shown at the top of the article. The researchers also launched the IMO Grand Challenge, hoping to bring AI to the IMO gold medal.\n(There is a hole to fill here, Mizar is very readable, very natural language like, and there is a recent rewritten version of Rust, I don\u0026rsquo;t know why it hasn\u0026rsquo;t caught on like Coq or Lean, wait for anyone who meets Mizar group to ask) The original motivation for mathematicians to develop formal languages was to test proofs programmatically and mechanically, but perhaps the most profound change this tool brought to the study of mathematics was that it clearly divided the study of mathematics into two parts: the form, the strict statement that defines the theorem and the calculus of the proof; And ideas, that is, why the definition theorem is written the way it is, why a proposition is used in a proof. Ideas are the most mysterious and confusing things in mathematics, excellent ideas are often named genius, and even regarded as miracles, ordinary people can only admire, let alone use scientific means to study and reproduce these ideas. Category theory, for example, may be one of those attempts that belongs to mathematicians. Now we have the tools of cognitive science and the theory of artificial intelligence, and as the first section shows, the laws of mathematical ideas are being revealed step by step. In order to more clearly demonstrate the dichotomy between form and idea in mathematics, I have been thinking of an experimental project to write a complete mathematics textbook in a formal language. What can be written in formal language is all formalized, and there must be something left that must be written in natural language, and these are the parts of the idea. These natural languages can be a great source of research for ideas.\nThe most pressing problem facing DL+ATP research is that mathematical propositions expressed in formal language are still a drop in the ocean compared to the entire mathematical edifion of mankind. Fortunately, the development of LLM technology has led to \u0026ldquo;pre-training\u0026rdquo; and \u0026ldquo;fine-tuning\u0026rdquo; tools that can circumvent the lack of data to some extent. The idea is that many of the abilities and knowledge reflected in the data are universal, such as reasoning ability; The ability required for a particular task may be specialized, such as the specific syntax of a formal language. If the LLM has a general knowledge of the language before learning the domain-specific task, it is much better than the traditional method of directly letting the AI do the specific task from random initialization. Specifically, ATP, natural language mathematics, and formal language mathematics express the same mathematical meaning despite their differences in rigor and grammar, so the LLM pre-trained on a large number of human mathematical languages, learned some mathematical knowledge first, and then specialized in fine-tuning the scarce formal languages, should have a much better effect than only learning on formal language data. The classic work of DL+ATP, GPT-f, is to pretrain GPT3 on the natural language data set CommonCrawl, 23GB of Github code, 10GB of Arxiv Math papers, and 2GB of Math StackExchange forum articles. The motivation for training on code is that code also reflects human reasoning, much like proof. However, the subsequent improvement work of GPT-f HTPS believes that it is better to only pre-train on Arxiv, and who can say the configuration of optimized performance in engineering. HTPS has achieved a pass rate of 58.6% on the verification set and 41.0% on the test set of Lean language MiniF2F, which is the best result of pure proof search relying only on formal language. More information on evaluating data sets and algorithm performance can be found on the PaperWithCode page, but the information on this page is incomplete and has not been maintained for a long time.\nNote 1: One of the drawbacks of fine-tuning is that it causes the LLM to focus only on domain-specific tasks and forget about general tasks, so it cannot be mathematically fine-tuned by tools such as ChatGPT to give it strong mathematical capabilities, which would make it lose other capabilities. Mathematical data sets were already part of ChatGPT\u0026rsquo;s pre-training, although the pre-training data did not give it enough mathematical power. Although LLMS can be pre-trained to learn mathematics beyond the available theorem proof data, the lack of theorem proof data is always a major hindrance to advancing the field. Readers who have the experience of using formal language will have a deep understanding that the process of mathematical proof in formal language is much more difficult than that of writing proof in natural language. Many details that we usually think are very ordinary and natural need to be completed, so the establishment of formal language theorem library needs a lot of human labor. If writing a formal language is such a hassle, why not let AI do it? The first attempt was made in 2018, followed by a report by Christian Szegedy in 2020, which formally raised the issue of autoformalization, the automatic translation of natural language mathematical propositions and proofs into formal language, like machine translation. The best performance of automated formalization tasks to date is also achieved by LLM, with a May 2022 test reporting that the best programming language model, Codex, could correctly translate 38 problems from natural language to Isabelle out of 150 problems extracted from the MATH dataset (which contains 12,500 natural language expressed math problems). It can be imagined that an algorithm that can translate two different mathematical languages well will inevitably involve an understanding of the nature of mathematical semantics, so the study of automatic formalization will also feed into the study of proof.\nIn terms of the concrete realization of the two problems, automatic formalization and ATP are also complementary tasks, because natural language proof is full of leaps, these natural omissions for humans can not be handled in the formal system, then ATP can be used to automatically complete. In turn, there is a clever practical idea that since formal language data is scarce, the search process for ATP can also be guided by natural language propositions. Representative work in this direction is the 2022 Draft, sketch, and prove: Guiding formal theorem provers with informal proofs (DSP), which uses Codex on Isabelle language to map natural language proofs into formal language proofs \u0026ldquo;draft\u0026rdquo;, that is, there are some leap-forward formal proofs. These hops are then automatically completed with Isabelle\u0026rsquo;s symbolic autoprover Sledgehammer (Isabelle\u0026rsquo;s symbolic autoprover is much stronger than Lean\u0026rsquo;s). The natural language proofs could have been written by a human expert, who achieved 39.3% test set accuracy, or they could have been produced by another large model of natural language mathematical questions, Minerva, which achieved 38.9% test set accuracy. If Minerva is used, then both the natural language template and the formal language proof are produced by the algorithm itself, which can also be seen as the algorithm itself doing the ATP task. the best subsequent results from this method came from Decomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving, which is an improvement of DSP, also achieves 45.5% accuracy of the test set on Isabelle, which is also the best result on MiniF2F dataset. However, it is based on a different formal language from HTPS, and relies on natural language and cannot be directly indexed.\nNote: One possible challenge to the DSP is that the Minerva training data contains mathematical text collected on the web page and the Arxiv text, which does not necessarily exclude the MiniF2F questions and is not \u0026ldquo;clean\u0026rdquo; enough, but the large language model can be regarded as a compression of the data, and generally cannot \u0026ldquo;recite\u0026rdquo; the solution, but can be regarded as a re-generated solution. However, Microsoft Copilot has done a job of reproducing irrelevant program comments, and the LLM\u0026rsquo;s internal data processing principles are still harder to understand. These results are far from ideal, but they have now translated some ATP tools that mathematicians may not think they can use. 22 years LEAN-GPTF as a proof tactic in Lean open test for a period of time, can prove some simple theorems, now closed test. HTPS has been integrated into Lean\u0026rsquo;s VSCode editor extension, and when you write a proof you click a button to get the next sentence of a dozen AI recommendations, most of which are wrong, but some are right. VSCode\u0026rsquo;s Copilot programming assistant can occasionally translate natural language math statements you write into Coq or Lean, and ChatGPT can occasionally write correct formal language proofs. The current situation looks bleak, but remember that this direction has only begun to develop in the last two or three years.\nIf you are willing to give a few more years of patience, what will DL+ATP bring to the world? The most immediate is to accelerate the development of formal mathematics. Introduction to Computer-Aided Proofs shows us a reliable and fair picture of mathematical research, and ATP can greatly reduce the cost of realizing this vision. We can feed a human math paper or a student\u0026rsquo;s answer to a test question into an automated formal algorithm, and then have it generate code to verify correctness. Alternatively, you can translate the code into human language to make it easier to read, or extract concise ideas from complex proofs to help you understand them. We may not be able to make ATP system prove some big theorems for a long time, but it is very likely to assist in the process of human thinking about mathematics, for example, an important possibility is the auxiliary retrieval of mathematical content: An idea came to my mind, and I wondered if anyone else had come up with a similar idea, maybe there was a precise theorem that expressed the idea in a way that traditional keyword matching searches could not find, but AI could understand the idea at an abstract semantic level to find relevant data.\nFinally, a little sci-fi fantasy. If one day mathematical AI can prove the Riemann conjecture, it is not difficult to imagine that AI can also solve scientific problems such as room-temperature superconductivity, controlled nuclear fusion, eliminate various diseases, and even solve all social production problems. Mathematics represents the peak of human abstract reasoning ability, and when AI also has abstract reasoning ability, it may be able to become the complete general artificial intelligence. What will society look like then?\nLearning Map If you are an enthusiast who wants to learn more about this field, with the goal of generally reading the leading papers in this direction, then you will need some introduction to deep learning and a preliminary understanding of LLM, at least familiar with the principles of the encoder-decoder architecture. It is more important to have experience with at least one formal language, and I recommend Lean here because modern DL+ATP is increasingly using Lean.\nIf you want to pursue research in this field\u0026hellip;\u0026hellip; Dissuade warning! You\u0026rsquo;re dealing with the most complex and comprehensive field in AI and the second most distant from money after people who do theory. (If any friend is not satisfied, please contact me to help ATP improve the ranking, thank x in advance)\nDSP instructor Albert Jiang pointed out that DL4Math people have a lot of money. HTPS writers Guillaume, Tim, Thibault, Marie-Anne went to mistral ai, Google\u0026rsquo;s Yuhuai Wu, Christian went to xai, jesse founded morph, stanislas founded dust, markus is starting a business. But I refuse to admit it until someone gives me money (x). The first is the need for more comprehensive knowledge of deep learning and reinforcement learning. LLM Needless to say, the main paradigm today is based on LLM, but CV knowledge is also helpful, because geometric propositions and the geometric intuitions of some propositions also need to be taken into account; Graph-based approaches have potential, so an understanding of GNN-related approaches can also be helpful; The latest work even uses the popular Diffusion method. In addition to these modern techniques, as seen before, DL and RL are still developed under the framework built in the pre-deep learning era, so it is important to understand the traditional methods. Outside of AI, you\u0026rsquo;ll be working on the basis of a type theory language, so you\u0026rsquo;ll need to know some \u0026ldquo;common sense\u0026rdquo;, such as mathematical logic and various type theories, functional programming, category theory, and so on. Some subproblems with a strong symbolic tradition will emphasize this knowledge more, but work like HTPS will use it as background knowledge. Overall, the road ahead for ATP is not clear, and the accumulated knowledge in this field is not deep enough to get started quickly, but researchers must have a fairly wide range of knowledge to find the next breakthrough, although they may not be directly applicable, but can provide more insight or at least not cause you to make a simple mistake.\nThe main event in the field of DL+ATP is the Artificial Intelligence and Theorem Proving conference AITP, and the Intelligent Computer Mathematics Conference CICM is also noteworthy. In addition, the MATH-AI workshop has been held in ICLR2021 and NeurIPS2022 for two sessions, and the third session will be held in NeurIPS2023 at the end of this year (2023.12.15/16). Welcome to your attention. At the NeurIPS conference, Albert Jiang, Kaiyu Yang and Emily First will also provide tutorial on machine learning theorem proving. Welcome to join us! In addition to conferences, if you\u0026rsquo;re in the Lean zulip community, check out stream Machine-Learning-for-Theorem Proving.\nFinally, post a list of papers for further study.\nA survey of deep learning for mathematical reasoning (2022). An overview is always the best place to start.\nHolophrasm: a neural automated theorem prover for higher-order logic (2016). DL+ATP started with quite advanced ideas, and there was even no transformer at that time. Unfortunately, the author has retired now.\nGenerative language modeling for automated theorem proving (2020). The famous GPT-f OpenAI is in with a lot of money.\nProof artifact co-training for theorem proving with language models (2021). Another classic work PACT, which uses a new training method and is more robust than GPT-f, is the main control group for future work.\nFormal mathematics statement curriculum learning (2022). The highlight is the use of curriculum learning, which is stronger than PACT.\nHypertree proof search for neural theorem proving (2022). The strongest HTPS, GPT-f scale +Holophrasm search algorithm. OpenAI\u0026rsquo;s big money turned around to hit ChatGPT, Meta took over OpenAI, but it\u0026rsquo;s not open source.\nDT-Solver: Automated Theorem Proving with Dynamic-Tree Sampling Guided by Proof-level Value Function (2023). The latest achievement, produced by Sun Yat-sen University, Peking University and Huawei. Slightly better than PACT.\nLeanDojo: Theorem Proving with Retrieval-Augmented Language Models (2023). I do not want to list it, but this article has been on the public number for a long time, in fact, the main work is to establish a data set. The structure of the model proposed in the paper is simple, and the Reprover model feels like a matching validation of the data set.\nThere are also Skip-tree Training, TacticToe, LISA and other equally wonderful jobs.\nThe above is the most basic ATP work using pure formal language, and there are more natural language components below.\nThor: Wielding hammers to integrate language models and automated theorem provers (2022) This is Thor, Go beyond PACT by having a language model that is not fine-tuned on a formal language dataset work with the formal language editor\u0026rsquo;s automatic search function.\nSolving quantitative reasoning problems with language models (2022). This is Minerva, natural language math, and even physics and chemistry.\nDraft, sketch, and prove: Guiding formal theorem provers with informal proofs (2022). This is DSP, using natural language proofs to guide the standard work of formal language proofs, going beyond Thor.\nDecomposing the Enigma: Subgoal-based Demonstration Learning for Formal Theorem Proving (2023). in this paper, the Subgoal method of reinforcement learning is introduced, and the optimal result on MiniF2F is achieved by using Diffusion on the graph to optimize in context learning.\nAutoformalization with large language models (2022). This article is enough for the direction of pure breed automatic formalization, and for early research you can check the Related Work section of this article.\nEvaluating Language Models for Mathematics through Interactions (2023). A detailed survey report on the mathematical ability of ChatGPT class large language models.\nThis list is extensive, interested friends can follow the guide in the above articles. There may be a lot of important work I can\u0026rsquo;t think of right now, please add in the comments section.\n","permalink":"https://subfish-zhou.github.io/posts/dlatp/","summary":"DL4ATP","title":"Introduction of Deep Learning for Automatic Theorem Proving"}]