[{"content":"My Profile Hi I\u0026rsquo;m Subfish Zhou! Studying AI4Math. My interest in AI4MATH began during my sophomore year, when realized the potential and profound impact of using AI to solve mathematical problems. The AI4MATH technology contributes to our understanding of intelligence, and the technology can directly increase human knowledge of mathematics, physics, and other natural sciences. Imagine how the world would change if AI could help us solve Riemann Hypothesis or create the controlled fusion technology!\nMy work Article:\nIntroduction of Deep Learning for Automatic Theorem Proving\nTheorem proving in Lean4 (translation)\nMathematics in Lean4(translation)\nReport:\nProgress in automatic theorem proving of LLM\nHow to think about a thinking machine\nConsciousness, knowledge, ways of thinking, and general artificial intelligence\n","permalink":"https://subfish-zhou.github.io/posts/myprofile/","summary":"Who is Subfish","title":"My Profile"},{"content":"Diffusion 本文是Google发布的Understanding Diffusion Models: A Unified Perspective一文的笔记。\n生成模型简介 给定我们关注的数据分布中的观测样本$x$，生成模型的目标是学习真实的数据分布$p(x)$。这样之后就可以从近似模型任意生成新的样本，以及使用近似模型来评估样本数据的概率。\n目前有几个较重要的方向，下面简要概括：\n生成对抗网络(GANs)模拟了一个复杂分布的抽样过程，它是通过对抗的方式学习的。 “基于似然的模型”，将高似然概率分配给观测样本。这包括自回归模型、正规流（normalizing flows）和变分自编码器(VAE)。 基于能量的模型，分布被学习为任意可变的能量函数，然后被归一化。 基于分数的模型，与能量模型很相关：它将基于能量的模型的分数作为神经网络来学习。 本文介绍扩散模型，它有基于似然和基于分数的解释。会非常数学，不过这些推导很细节，做好准备！\n背景：ELBO，VAE和分层VAE 证据下界（Evidence Lower Bound） 对于很多模式中的数据，可以看作是由未知的隐变量$z$产生出的。把隐变量$z$和数据$x$建模成联合概率分布$p(x,z)$。基于似然的模型最大化所有观测$x$的可能性$p(x)$，可以用两种方法操纵联合分布排除隐变量$z$来恢复$p(x)$： $$p(x)=\\int p(x,z)dz\\tag{1}$$\n$$p(x)=\\frac{p(x,z)}{p(z|x)}\\tag{2}$$\n直接计算以及最大化似然函数$p(x)$是困难的，因为积分和条件概率的计算都比较难。我们用一个近似的变分分布$q_{\\phi}(z|x)$来估计后验概率$p(z|x)$，然后去优化最大似然的对数（这里被称为证据Evidence）：\n$$ \\begin{aligned} \\log p(\\pmb{x}) \u0026amp;=\\log p(\\pmb{x}) \\int q_{\\phi}(\\pmb{z} | \\pmb{x}) d z \\ \u0026amp;=\\int q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})(\\log p(\\pmb{x})) d z \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}[\\log p(\\pmb{x})] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{p(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z}) q_{\\phi}(\\pmb{z} | \\pmb{x})}{p(\\pmb{z} | \\pmb{x}) q_{\\phi}(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})}\\right]+\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{q_{\\phi}(\\pmb{z} | \\pmb{x})}{p(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})}\\right]+D_{\\mathrm{KL}}\\left(q_{\\phi}(\\pmb{z} | \\pmb{x}) | p(\\pmb{z} | \\pmb{x})\\right) \\ \u0026amp; \\geq \\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\phi}(\\pmb{z} | \\pmb{x})}\\right] \\end{aligned} $$\n因为$q_{\\phi}(z|x)$和$p(z|x)$的DL散度$D_{\\mathrm{KL}}\\left(q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x}) | p(\\pmb{z} | \\pmb{x})\\right) \\geq 0$，最后一行得到的期望被称为证据下界（ELBO），透过不等式，最大化ELBO就等价于最大化似然和最小化DL散度（注意，ELBO是关于$\\phi$的函数）。\n如何实现ELBO的最大化，亦即如何实现一个好的$q_{\\phi}(z|x)$，将我们引到了VAE之中。\n变分自编码器（Variational AutoEncoder） 自编码器是一个学习输入数据$x$，获得中间表示$z$，然后再拿来预测$x$自己的模型。$q_{\\phi}(z|x)$就是编码器，$p_{\\theta}(x|z)$就是解码器。说它是变分的，是因为$q_{\\phi}(z|x)$从一族由$\\phi$参数化的隐后验分布中优化而来。\n","permalink":"https://subfish-zhou.github.io/posts/diffusion/diffusion/","summary":"Diffusion 本文是Google发布的Understanding Diffusion Models: A Unified Perspective一文的笔记。\n生成模型简介 给定我们关注的数据分布中的观测样本$x$，生成模型的目标是学习真实的数据分布$p(x)$。这样之后就可以从近似模型任意生成新的样本，以及使用近似模型来评估样本数据的概率。\n目前有几个较重要的方向，下面简要概括：\n生成对抗网络(GANs)模拟了一个复杂分布的抽样过程，它是通过对抗的方式学习的。 “基于似然的模型”，将高似然概率分配给观测样本。这包括自回归模型、正规流（normalizing flows）和变分自编码器(VAE)。 基于能量的模型，分布被学习为任意可变的能量函数，然后被归一化。 基于分数的模型，与能量模型很相关：它将基于能量的模型的分数作为神经网络来学习。 本文介绍扩散模型，它有基于似然和基于分数的解释。会非常数学，不过这些推导很细节，做好准备！\n背景：ELBO，VAE和分层VAE 证据下界（Evidence Lower Bound） 对于很多模式中的数据，可以看作是由未知的隐变量$z$产生出的。把隐变量$z$和数据$x$建模成联合概率分布$p(x,z)$。基于似然的模型最大化所有观测$x$的可能性$p(x)$，可以用两种方法操纵联合分布排除隐变量$z$来恢复$p(x)$： $$p(x)=\\int p(x,z)dz\\tag{1}$$\n$$p(x)=\\frac{p(x,z)}{p(z|x)}\\tag{2}$$\n直接计算以及最大化似然函数$p(x)$是困难的，因为积分和条件概率的计算都比较难。我们用一个近似的变分分布$q_{\\phi}(z|x)$来估计后验概率$p(z|x)$，然后去优化最大似然的对数（这里被称为证据Evidence）：\n$$ \\begin{aligned} \\log p(\\pmb{x}) \u0026amp;=\\log p(\\pmb{x}) \\int q_{\\phi}(\\pmb{z} | \\pmb{x}) d z \\ \u0026amp;=\\int q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})(\\log p(\\pmb{x})) d z \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}[\\log p(\\pmb{x})] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{p(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z}) q_{\\phi}(\\pmb{z} | \\pmb{x})}{p(\\pmb{z} | \\pmb{x}) q_{\\phi}(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})}\\right]+\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{q_{\\phi}(\\pmb{z} | \\pmb{x})}{p(\\pmb{z} | \\pmb{x})}\\right] \\ \u0026amp;=\\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\pmb{\\phi}}(\\pmb{z} | \\pmb{x})}\\right]+D_{\\mathrm{KL}}\\left(q_{\\phi}(\\pmb{z} | \\pmb{x}) | p(\\pmb{z} | \\pmb{x})\\right) \\ \u0026amp; \\geq \\mathbb{E}{q{\\phi}(\\pmb{z} | \\pmb{x})}\\left[\\log \\frac{p(\\pmb{x}, \\pmb{z})}{q_{\\phi}(\\pmb{z} | \\pmb{x})}\\right] \\end{aligned} $$","title":"【Note】Diffusion"},{"content":"General Visual Language Model 本文译自Lilian Weng的文章Generalized Visual Language Models，在此感谢她的辛勤创作，献上花花🌸。由于翻译量较大，机翻了部分段落；还有一些术语不知道怎么翻译或者翻译出来怪怪的，都附上英文原文。译者能力有限，恳请读者海涵。\n处理图像生成文本，例如看图说话和视觉QA，已经研究了好多年。传统上，这种系统依赖于对象检测网络作为视觉编码器来捕获视觉特征，然后通过文本解码器产生文本。本文总结了现今的不少论文，专注于解决视觉语言任务的一种方法，即扩展预训练的一般语言模型，使其能够处理视觉信号。\n我将这种视觉语言模型(VLMs)大致分为四类:\n将图像转换为可与文本token嵌入联合训练的嵌入特征。 学习好的图像嵌入，可以作为固定的、预训练的语言模型的前缀。 使用特别设计的交叉注意机制将视觉信息融合到语言模型的各个层中。 结合视觉和语言模型，无需任何训练。 图像和文本联合训练 将视觉信息融合到语言模型中的一种直接方法是将图像视为普通的文本token，并在文本和图像的联合表示序列上训练模型。准确地说，图像被分成许多小块(patch)，每个小块被视为输入序列中的一个token。\nVisualBERT(Li et al. 2019)将文本输入和图像区域都输入到BERT中，这样它就能够通过自注意力发现图像和文本之间的内部对齐。\n图1：VisualBERT是结合文本和图像嵌入进行训练的。图源：[(Li et al. 2019)](https://arxiv.org/abs/1908.03557)\n类似于BERT中的文本嵌入，VisualBERT中每个视觉嵌入也是由三种类型的嵌入加和而成的，分别是tokenized特征$f_o$，识别嵌入(segment embedding) $f_s$和位置嵌入$f_p$。具体地：\n$f_o$是觉特征向量，由CNN在一个边界区域上计算得来； $f_s$是识别嵌入，用于识别嵌入来自视觉还是文本； $f_p$是位置嵌入，用于给边界区域排顺序。 该模型在MS COCO图像标题数据集上训练，文本和图像都作为输入来预测文本描述(caption)，使用两个基于视觉的语言模型目标：\n含图像的掩码语言模型MLM。该模型需要预测文本tokens中的掩码，而图像嵌入始终保持不掩码。 句子-图像预测。提供一张图片和两个描述句子，其中一个句子是图片相关的，另一个句子有50%概率相关50%概率随机。模型被要求区分这两种情况。 根据消融实验，最重要的配置是将视觉信息早早融合到transformer层，并在COCO caption数据集上预训练模型。从一个预训练好的BERT初始化以及采用句子-图像预测训练目标，带来的影响相对较小。\n图2：VisualBERT在NLVR上的消融实验结果。图源：[(Li et al. 2019)](https://arxiv.org/abs/1908.03557)\nVisualBERT在NLVR和Flickr30K上的性能优于SoTA，但在VQA上仍与SoTA存在一定的性能差距。\nSimVLM(Simple Visual Language Model；Wang et al. 2022)是一个简单的前缀语言模型，其中前缀序列像BERT一样用双向注意力处理，但主输入序列像GPT一样只有因果注意力。图像被编码为前缀token，这样模型就可以完全采纳视觉信息，然后以自回归的方式生成相关的文本。\n受ViT和CoAtNet的启发，SimVLM将图像分割成更小的块，并展成一维小块序列。他们使用由ResNet的前3个块组成的卷积步骤来提取包含上下文信息的小块，这种设置被发现比平凡的线性投影更好。\n图3：SimVLM的训练架构，其中图像小块由交叉注意力编码器处理，文本解码器具有因果注意力。图源：[Wang et al. 2022](https://arxiv.org/abs/2108.10904)\nSimVLM的训练数据包括来自ALIGN(Jia et al. 2021)的大量图像-文本对和来自C4数据集(Raffel et al. 2019)的纯文本数据。他们在每个小批量(batch)中混合了两个预训练数据集，包含4096个图像-文本对(ALIGN)和512个纯文本文档(C4)。\n根据消融实验，同时拥有图像-文本和纯文本数据进行训练是很重要的。前缀语言模型目标优于span corruption和普通语言模型。\n图4：SimVLM在VQA上的消融实验结果。图源:[(Wang et al. 2022)](https://arxiv.org/abs/2108.10904)\n","permalink":"https://subfish-zhou.github.io/posts/generalized_visual_lm/","summary":"summary","title":"【Translate】General Visual Language Model"},{"content":"Introduction of Deep Learning for Automatic Theorem Proving Basic mathematics is finally waiting for a day when it can be computer-aided to massively increase productivity. In recent years, the field of automatic theorem proving (ATP) has generated an amazing amount of work through the introduction of deep learning and reinforcement learning, especially with the rapid progress of Large language models (LLM). Although we are still a little far away from having computers prove big propositions like the Riemann conjecture on their own, we can already talk predictably about how automatic theorem proving will help human mathematicians do their job. This paper pays tribute to the influential @Peng Keyao \u0026ldquo;Introduction to Computer-Aided Proof\u0026rdquo;. The formal mathematical language introduced in this paper is also the theoretical background of this paper. Modern ATP system is basically carried out on the basis of formal language. It is also shown as tactic in Lean or hammer in Isabelle, so it is suggested that readers who have not read this article should refer to it first.\nThis article focuses on the current progress in automatic theorem proving with deep learning and reinforcement learning (abbreviated DL+ATP below, but note that RL is also key). DL+ATP is developing rapidly, but I don\u0026rsquo;t want this article to be out of date, so I won\u0026rsquo;t cover too much technical content, and I will keep up with the field with another review that will be updated continuously. The purpose of this article is to present ideas, implications, clarify misconceptions, and answer doubts, with the aim of exposing this new, vibrant and significant field to enthusiasts and researchers in mathematics, computing, and beyond.\nThis article begins with the following questions (but the answers are intertwined, so the table of contents is not organized there) :\nWhy theorems can be proved automatically;\nHow DL+ATP works;\nWhat achievements have been made in the field of automatic proof;\nWhat mathematicians hope to gain from automatic proof;\nWhat are the effects of automatic theorem proving;\nBeginners how to get started with DL+ATP.\nHere are some of the results so far: As of the writing time of this paper on July 1, 2023.1, the most representative and influential DL+ATP work HyperTree Proof Search for Neural Theorem Proving (2022.5.23, see Meta Blog, paper, HTPS), automatically solved 10 International Mathematical Olympiades (IMO) problems, such as one of them:\n（IMO1964p1_2）7 can\u0026rsquo; t divide $2^n+1$ exactly.\nThe formal expression of this problem in Lean and the automatic proof process given by AI are as follows:\nimport data.nat.prime\rtheorem imo_1964_p1_2 (n : ℕ) :\r¬ 7 ∣ (2 ^ n + 1) :=\rbegin\rrw nat.dvd_iff_mod_eq_zero,\rrewrite [nat.add_mod, nat.mod_eq_of_lt],\robviously,\rapply nat.strong_induction_on n,\rinduction n,\r{\rintros n IH,\rcases n,\rnorm_num,\rcases n,\rnorm_num,\rrw [nat.succ_eq_add_one, pow_succ],\rrw [nat.succ_eq_add_one, pow_succ],\rinduction n,\rnorm_num,\rrw [nat.succ_eq_add_one, pow_succ],\rnorm_num [nat.mul_mod, ←mul_assoc],\rcontrapose! IH,\rrefine ⟨n_n, nat.lt_succ_iff.mpr _, IH⟩,\rexact nat.le_succ_of_le (nat.le_succ _),\r},\rexact n_ih,\rend This automatically generated proof (after a lot of simplification) translates into natural language roughly as follows:\nThe original problem is equivalent to $2^n+ the remainder of $1 divided by 7 is not equal to 0, so either $2^n divided by the remainder of 7 plus the remainder of 1 divided by 7 is not equal to 0, which is obvious, or $2^n divided by the remainder of 7 plus the remainder of 1 divided by 7 is less than 7. This proposition is proved by strong induction ∀(n :ℕ),(∀(m :ℕ),m \u0026lt; n → 2 ^ m % 7 + 1 \u0026lt; 7) → 2 ^ n % 7 + 1 \u0026lt; 7 inductive 1)∀ (m : ℕ), m \u0026lt; 0 → 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ 0 % 7 + 1 \u0026lt; 7 : obviously 2)∀ (m : ℕ), m \u0026lt; n + 1 → 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (n + 1) % 7 + 1 \u0026lt; 7 inductive 1.0) ∀ (m : ℕ), m \u0026lt; 1 → 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ 1 % 7 + 1 \u0026lt; 7 : obviously 1.1) ∀ (m : ℕ), m \u0026lt; n + 1 + 1 → 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (n + 1 + 1) % 7 + 1 \u0026lt; 7 inductive 1.1.0) ∀ (m : ℕ), m \u0026lt; 1 + 1 → 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (1 + 1)% 7 + 1 \u0026lt; 7 : obviously 1.1.1) ∀ (m : ℕ), m \u0026lt; n + 1 + 1 + 1→ 2 ^ m % 7 + 1 \u0026lt; 7 pf. 2 ^ (n + 1 + 1 + 1) % 7 + 1 \u0026lt; 7 So far, 2 ^ (n + 1 + 1 + 1) % 7 + 1 = 8 * 2 ^ n % 7 + 1 = (7 + 1) * 2 ^ n % 7 + 1 = 2 ^ n % 7 + 1, obviously.\nIf we look closely, we will find that this proof through extremely violent, with a strong mechanical wind means, in fact, also in disguised realization of the human proof \u0026ldquo;first observed\u0026rdquo; this \u0026ldquo;fantastic idea\u0026rdquo; (TL; DR, mainly reflected in the last step). It should be pointed out that this proof is not generated by a fixed algorithm of symbolism, such as SAT or Wu method, but is indeed actively searched out in the proof tree by thinking in a similar way to human mathematicians (algorithmic ideas will be briefly introduced in the following article). However, although the principle of this algorithm is close to human thinking, the result of this proof is really not like human writing, tedious and unelegant, mathematicians may feel that the rice bowl is still secure after reading. But this result also makes us reflect that we may indeed have come to the moment when we can discuss AI automatic theorem proving, although the \u0026ldquo;observation\u0026rdquo; of this problem is relatively simple, slightly trained students can master, but is the \u0026ldquo;art\u0026rdquo; of the theorem proving considered more profound really a kind of unattainable light of human wisdom, which is not allowed to be touched by AI?\nNote: You may have heard of LeanDojo (the work of the new intelligent Yuan scam), but the paper does not see that it is more powerful than Hypertree. We\u0026rsquo;ll talk about that later.\n自动证明是如何可能的 Automatic theorem proving is one of the earliest and most traditional problems in artificial intelligence, as far back as 1954, Martin Davis developed the first theorem proving program, proving that \u0026ldquo;the sum of two even numbers is even\u0026rdquo;; The Logic theorist program presented by Allen Newell and Herbert Simon at the Immortal Dartmouth Conference in 1956 proved more than half the theorems in Russell and Whitehead\u0026rsquo;s Principia Mathematica, and some were even more concise than the authors\u0026rsquo; proofs. Thus the school of symbols was founded. The influence of these early works on computer science is numerous, such as formal verification, expert systems and knowledge graphs are its descendants, and even the study of the SAT opened the discipline of computational complexity. The pinnacle of the symbolist line was the Vampire program, which is still being updated today and continues to play a role in areas such as formal verification. However, for the original goal of automatic proof of mathematical theorems, the symbolist route is inevitably limited by combinatorial explosion, so it can not be applied to the practice of mathematicians for a long time. In the statistical learning era, support vector machines and other models have been used to construct heuristics for proof search, but the results are not good. In recent years, with the addition of tools such as deep learning to automatic theorem proving, we finally have the hope that deep learning can develop heuristics good enough to overcome the combinatorial explosion and even achieve theorem proving algorithms that are close to the way humans think.\nBefore we look at how DL+ATP is possible, let\u0026rsquo;s look at how proof is structured as a computational task. There is no need to deal with the writing of successive calculus in proof theory, and we can ignore the concrete rules of calculus such as the resolution principle, and just from the most abstract task structure, we can think of the proof as a search tree. In analogy to the game of Go, each step has a number of points to choose, and eventually there will be several paths leading to victory, theorem proving can also be seen as in each propositional state (in the proof theory and ITP system, the states are context and goal) can apply a number of theorems or proof skills to the current state, so as to make it become the next state. And eventually there will be several paths leading to the completion of the proof. We use an example to show how search trees and the simplest heuristics work: the proof that \u0026ldquo;sqrt(2) is irrational\u0026rdquo;. If the reader is familiar with how ITP systems like Lean work, this example may be better understood.\nGiven this proposition, we have several options, for example, we can try induction and find that the proposition contains nothing that can be induced, so there is no way to perform induction. So maybe we can try proof by contradiction? Then we find that the proof by contradiction does change the state of the problem, and after restoring the definition back (that is, rewriting the definition in Lean) we get a new proposition \u0026ldquo;There are m,n, m,n mutual primes and sqrt(2)=m/n\u0026rdquo; and the goal becomes the derivation contradiction. Then there will be many choices, such as \u0026ldquo;both sides of the equation can be +1 at the same time\u0026rdquo; to apply, or \u0026ldquo;both sides of the equation can be multiplied by 2\u0026rdquo; to apply, or \u0026ldquo;both sides of the equation can be squared at the same time\u0026rdquo; to apply. If we don\u0026rsquo;t know the answer in advance, there are actually an infinite number of choices, that is, the search tree is infinitely wide. (In Lean, we have a lot of alternative tactics and theorems that can be rw) At this time, we have begun to see the \u0026ldquo;combinatorial explosion\u0026rdquo;, and the automatic proof algorithm cannot go through so many theorems, so some heuristics are needed to help select the next search branch. For example, you can use the number of symbols contained in the proposition to define a \u0026ldquo;proposition complexity\u0026rdquo;, and try to prevent the complexity of the proposition from rising when searching. In this way, the algorithm may give up trying to \u0026ldquo;+1 on both sides\u0026rdquo; because it makes the proposition more complicated. A less naive and more targeted heuristic at the moment might be \u0026ldquo;familiarity with symbols\u0026rdquo;, such as when the algorithm finds that it knows very little about sqrt, that only a few theorems can deal with it, and that more theorems can deal with integers and rational numbers, then it will tend to find a way to eliminate SQRT, at the moment using \u0026ldquo;squares of both sides\u0026rdquo;. Thus we (skipping a step) turn the original statement into \u0026ldquo;There are m,n, m,n mutual primes and 2=m²/n²\u0026rdquo;. Then the algorithm decides that it might not be that familiar with fractions, so it multiplies n² again, so the heuristic may stop there, and the algorithm will consider other search techniques.\nDue to Godel\u0026rsquo;s incompleteness theorem, Tarski\u0026rsquo;s first-order real numbers are decidable, but the super-exponential algorithm, and the NPC property of SAT, it can be argued that there is no \u0026ldquo;ultimate heuristic\u0026rdquo; that can guide all proofs, and Wu\u0026rsquo;s method that can completely solve elementary geometric problems is only a few special cases. In a general problem a heuristic can only be applied to a very narrow set of problem patterns. Theorem-prover in the era of symbolism is a big competition of heuristics, whose heuristic design is more delicate, who can better coordinate multiple heuristics, who can achieve better performance. But math problems are so varied that, even after decades of hard work, these provers are prohibitive about high school math, let alone keeping up with modern math.\nIn fact, other areas of AI have encountered similar situations, such as image recognition, which also relies on humans to manually design features, but performance on the Imagenet dataset has been hovering around 50%. In 2014, deep learning was born and instantly changed the face of the image recognition field, and now deep learning has also begun to emerge in the field of theorem proving.\nWhy is all this possible? You may feel that symbolic heuristic design may be in line with some human ideas in some cases, but it does not capture the most crucial part of human mathematical practice: humans have gone through a considerable amount of mathematical training, accumulated a lot of experience, and sometimes these experiences are difficult to describe, \u0026ldquo;I don\u0026rsquo;t know how I came up with it, but I just thought of it.\u0026rdquo; Deep learning can precisely allow the algorithm to accumulate experience, when it has done many many problems, when it encounters a new problem, it will recall whether it has seen a similar \u0026ldquo;problem type\u0026rdquo;, and find a similar practice with the training at that time.\nWe can use Monte Carlo search trees to describe a statistically enhanced theorem proving model. The Monte Carlo search tree gives a probability for each branch, such as 0.9 for \u0026ldquo;proof by contradiction\u0026rdquo;, 0.09 for \u0026ldquo;induction\u0026rdquo;, and 0.01 for some branches. The search is prioritized by probability, which avoids traversing all branches. In the symbolist era, in fact, the combined action of multiple heuristics would also use this model, but under a fixed combination of symbolic heuristics, the probability is usually fixed, but now we can adjust these probabilities through training (this is actually a reinforcement learning technique), For example, after searching for a branch with 0.9 probability, the algorithm finds that the descendants of this branch cannot successfully prove the theorem, indicating that the algorithm is wrong in estimating the success of this method, and then it will appropriately reduce this probability to avoid repeating the mistake (only reduce rather than completely kill it). The descendants of this branch may not be completely traversed and therefore it is not entirely certain that it is not possible). The HTPS algorithm introduced at the beginning of this article goes a step further on this idea, and it finds that people will have relatively fixed problem-solving routines when they encounter some problem types, that is, a series of theorem application steps may appear repeatedly, and only a few parameters may change according to specific circumstances. It then packages these routines into trees that become alternatives at each search step, eliminating the need to search one by one for theorems to be applied next. This \u0026ldquo;tree on tree\u0026rdquo; idea is the origin of \u0026ldquo;Hypertree\u0026rdquo;.\nThe probabilistic feedback mechanism of the Monte Carlo search tree alone is not enough, because the tree is too large (infinite), most of the solutions can not be found, so it is necessary to accumulate experience from training through deep neural networks, which are given a good enough initial probability in the Monte Carlo tree model. What deep learning does best is capture features, such as features of cats and dogs in images, or features common to English and Chinese. In the most abstract sense, analytical theorems are more likely to be proved analytically, and algebraic theorems are more likely to be solved with algebraic tools. Even if algebraic theorems are really proved analytically, we can also feel some \u0026ldquo;analytical flavor\u0026rdquo; in them. These are actually a kind of restriction on the direction of search. Deep neural networks can \u0026ldquo;accumulate experience\u0026rdquo; by discovering the features of proposition or proof process in the data, and adopting the corresponding proof process when encountering propositions with similar characteristics.\nFeatures in different task data often have their own uniqueness, and we can better extract them by designing networks with different structures accordingly. For example, we use convolutional neural networks to capture features in images. So what kind of deep neural network should we design to extract the features in the proof? Some earlier work had treated proofs as sequences or graphs, and accordingly used RNNS or GNNS to process them, but with little success. The recent success of large language models has led us to think of proofs as a language, and to use language models to capture features. The GPT-f family of work represented by HTPS uses a language model similar to GPT2, while the more recent LeanDojo uses a T5 model. The details of how these language models work and how different language models differ in proving theorems are outside the topic of this article. Readers are free to inquire for their own information, which will be covered in a technical overview later in this article.\nBut we also see that training theorem proving on LLM is not good enough. Part of LeanDojo\u0026rsquo;s work shows that GPT4 cannot prove complex mathematical theorems such as Stirling\u0026rsquo;s formula, even with sufficient prompt. The reason may be that there are too few theorem-proof data, and the most suitable method of LLM training has not been found, resulting in the lack of full use of LLM capabilities; It is also possible that the task of theorem proving itself has a richer complex and fine structure than language, and the violent aesthetics of LLM are not enough to conquer the arduous task of theorem proving. But while the task is daunting, it is not as mysterious as it was a few decades ago, and the mechanisms of proof and even reasoning have accumulated a wealth of knowledge, but there is still a lot of work to be done.\nThis section concludes with a response to some common questions:\nBecause of Godel\u0026rsquo;s incompleteness theorem (or for some other reason), a machine can never prove a mathematical theorem, because people are not bound by Godel\u0026rsquo;s theorem, so theorem proving can only be done by people.\nResponse: Godel actually has this view himself. But this view implies that man is an oracle Turing machine, that is, a Turing machine that can run for an infinite amount of time, and can travel through time to inform the present moment of the results of its operation after an infinite amount of time. According to the Church-Turing thesis, the Oracle is not a computational model that can be implemented in our universe. Corresponding to the task of proving theorems, this means that one can go through all the possibilities and find the right theorems to apply to without wasting time. It seems that human \u0026ldquo;inspiration\u0026rdquo; has a similar quality, but it actually means that one can find a proof without mathematical training, just by dreaming of a fairy. I prefer to believe that inspiration comes from some specific computational mechanism, even though it is currently only incompletely described by theories such as deep learning and reinforcement learning.\nBut people can write axiom systems such as ZFC, they can also know that these axiom systems are not complete and then add axioms, they also invented the Turing machine and know that the Turing machine cannot solve the halting problem, people can set the foundation for mathematical proofs and Turing machines, and the machine itself has no way to set the foundation for itself.\nResponse: This is to describe the level at which one can work on a metalinguistic level, that is, we study the properties of another language by virtue of language and only by virtue of language. A metalanguage is also a language, subject to the rules of computation, and presenting an axiomatic system and a halting problem is no different from other Turing-computable problems. Besides, people can\u0026rsquo;t solve the shutdown problem. There is no reason to think that Turing machines cannot observe themselves, just as modern computers can also check their own memory for errors, it is simply engineering.\n","permalink":"https://subfish-zhou.github.io/posts/dlatp/","summary":"DL4ATP","title":"Introduction of Deep Learning for Automatic Theorem Proving"},{"content":"自动形式化的问题背景 Therefore it is natural to ask: Will we ever arrive at the point where an AI agent can learn to do reasoning as well as the best humans in the world in most established domains of mathematics. \u0026mdash;- Christian Szegedy\nThis paper refers to A Promising Path Towards Autoformalization and General Artificial Intelligence\nIt is well known that deep learning is not very good at reasoning, and it is natural for us to think that we can test reasoning AI mathematically. But before entering into the problem, there are two obvious difficulties: (1) humans express mathematical knowledge in natural language, and the fuzziness of natural language makes it difficult for computers to understand and verify it in a mechanized way; (2) Human mathematical knowledge is scattered in a vast number of papers, and it is difficult to generate effective data sets for system training.\nAt the same time, the development of human basic mathematics has also encountered an obvious bottleneck, the complexity of human mathematical papers is getting higher and higher, resulting in the difficulty of verifying their correctness, often a paper review time is extremely long or even unable to review the situation.\nIn response to these problems, some mathematicians have developed formal mathematical languages to represent mathematical knowledge, and these formal languages represent propositions and proof processes that can be verified by fixed procedures. These mathematicians hope to verify the correctness of human mathematical knowledge and promote the development of human mathematics through computers, but the workload of translating human mathematics into formal language is very huge, and the content of the mathematical theorem library of the main formal language is very limited, and even cannot completely include the content of undergraduate mathematics.\nTherefore, the need to develop a mathematical automatic formalization algorithm is particularly urgent, on the one hand, it can provide a large number of clear mathematical knowledge base for mathematical reasoning artificial intelligence, on the other hand, it can verify the correctness of human mathematical knowledge on a large scale.\n简史 自动形式化的想法是由John McCarthy在1961年首次提出的。另一个早期尝试是1990年Donald Simons的博士论文。2004年Claus Zinn的博士论文中进行了第一次彻底的研究。这些工作并没有产生哪怕是部分实用的解决方案。\nJosef Urban在21世纪初开始研究这个课题。他设计了第一个大规模的大型理论推理基准，其动机是认为在大型数学事实知识库中进行推理是任何自动形式化系统的关键组成部分。2007年，他发表了用于大型理论推理的开创性的MaLARea系统。从那时起，他和Cezary Kaliszyk一直在带头研究大型理论的推理和自动格式化。\n参考 [1]First experiments with neural translation of informal to formal mathematics [2]Exploration of neural machine translation in autoformalization of mathematics in Mizar\nPartial application of AI technology to mathematics Progress of GPT-f series models in automatic reasoning: some IMO difficulty problems can be done.\nRef. [1]Generative Language Modeling for Automated Theorem Proving [2]Mathematical Reasoning via Self-supervised Skip-tree Training [3]Proof Artifact Co-training for Theorem Proving with Language Models [4]Formal Mathematics Statement Curriculum Learning\nMachine learning guides human intuition and uses clustering algorithms to capture patterns in examples of knot theory and symmetric group theory, resulting in the discovery of two new theorems.\nRef. [1]Advancing mathematics by guiding human intuition with AI\nA neural network solver that can solve undergraduate mathematical problems by using Python\u0026rsquo;s symbolic computation package sumpy.\nRef. [1]A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More\nPossible technical paths I am currently working on a data set corresponding to Lean in natural language. After obtaining the data set, I will design the methods of automatic translation theorem and proof respectively. Translation of the theorem can generally be done using the transformer class model; The translation of proof should be a reinforcement learning task, which is not very clear for the time being.\nOr you can try an unsupervised approach. SeeUnsupervised Translation of Programming Languages\n","permalink":"https://subfish-zhou.github.io/posts/autoformalization/","summary":"一个新课题","title":"数学自动形式化：通向自动推理之路"}]